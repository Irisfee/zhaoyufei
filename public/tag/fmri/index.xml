<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fMRI | Yufei Zhao | 赵雨菲</title>
    <link>/tag/fmri/</link>
      <atom:link href="/tag/fmri/index.xml" rel="self" type="application/rss+xml" />
    <description>fMRI</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Yufei Zhao ©2021</copyright><lastBuildDate>Tue, 18 May 2021 16:00:28 -0800</lastBuildDate>
    <image>
      <url>/images/icon_hu8daf07698441511167650349ca463977_8870_512x512_fill_lanczos_center_2.png</url>
      <title>fMRI</title>
      <link>/tag/fmri/</link>
    </image>
    
    <item>
      <title>Decoding memory content from human parietal cortex: VGG16 application on memory research</title>
      <link>/project/nsd/</link>
      <pubDate>Tue, 18 May 2021 16:00:28 -0800</pubDate>
      <guid>/project/nsd/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;When a stimulus from the past is re-encountered, this can elicit increased neural activation relative to a novel stimulus (repetition enhancement) or decreased activation (repetition attenuation). Studies of episodic memory have consistently found that repetition enhancement occurs within parietal cortex and that these enhancement effects are related to behavioral expressions of successful episodic remembering. Repetition attenuation, in contrast, is more typically observed in regions of occipitotemporal cortex and is less consistently related to behavioral expressions of episodic memory. Separately, pattern-based fMRI studies have found that information about the content of stimuli is reflected in both parietal cortex and ventral temporal cortex, but it is less clear how or whether these content representations are integrated with repetition-related memory signals.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;In this study, we utilize the &lt;a href=&#34;http://naturalscenesdataset.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Scene Dataset (NSD)&lt;/a&gt;, which is 7T fMRI study with data size around &lt;strong&gt;2+ TB&lt;/strong&gt;. In this dataset, eight participants performed a continuous recognition task spanning 30-40 fMRI scan sessions and up to 10,000 unique, naturalistic images.
&lt;img src=&#34;fig1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;vectorize-memory-content&#34;&gt;Vectorize memory content&lt;/h3&gt;
&lt;p&gt;In the study, the stimuli subjects remembered were images from &lt;a href=&#34;https://cocodataset.org/#home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COCO dataset&lt;/a&gt;. Thus, to vectorized the memory content information, we can extract information from each image.&lt;/p&gt;
&lt;h4 id=&#34;semantic-models&#34;&gt;Semantic models&lt;/h4&gt;
&lt;p&gt;Since each image from COCO dataset are annotated by human workers with English, we started by analyzing the image content with two popular semantic models: &lt;a href=&#34;https://arxiv.org/abs/1301.3781&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word2vec&lt;/a&gt; and &lt;a href=&#34;https://github.com/oborchers/Fast_Sentence_Embeddings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast sentence embedding (fse)&lt;/a&gt;. However, after comparing the similarity analysis based on the embedding from these two semantic models and our human raters on a small subset of the images, we were not satisfying with these two methods in terms of capturing the image content information.&lt;/p&gt;
&lt;h4 id=&#34;vgg16&#34;&gt;VGG16&lt;/h4&gt;
&lt;p&gt;We believe the undesirable results from the semantic models are due to the quality of the annotation: each image was annotated by five human workers, and each worked wrote one single sentence to describe the image. With this setting, the richness of the annotation is very limited so that the annotation is not representative to the image content. To solve this problem, we believe quantifying the image content directly from the visual part of the image would be a closer approximation of the memory content. After literature review, we decided that &lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VGG16&lt;/a&gt;, a convolutional neural network designed for image content classification and detection, would be a good candidate for our purpose. We passed the images through VGG16 and used the output from the last fully connected layer (fc3) as the content vector of each image. The VGG16 model was trained on Imagenet, which is a different image dataset from COCO. To make sure the difference of image dataset won&amp;rsquo;t be a problem for adopting the neural network in our analysis, I visualized all the images in our experiment based on their VGG16 fc3 feature similarity with t-SNE (see figure below). As can been, the feature output from the last fully connected layer can successfully represent both local and global distance of image similarity. Although the model learned from low level visual features, it can still capture high level information of the image. Here is a &lt;a href=&#34;/post/vgg-viusalization/&#34;&gt;post&lt;/a&gt; I wrote about how to extract features of VGG16 with pytorch and visualize images base on their similarity with t-SNE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;model-prediction&#34;&gt;Model prediction&lt;/h3&gt;
&lt;p&gt;We calculated the difference in multivoxel fMRI activations between the first presentation (initial encoding) and second presentation (retrieval) for each stimulus and tested whether these repetition-related differences in activity patterns carried information about the visual content of the image. As demonstrate previously, to quantify the content of each image, we passed the images through VGG16, a convolutional neural network designed for image content classification and detection. We then reduced features from the output layer to 10 PCA dimensions and used ridge regression to test whether repetition-related changes in fMRI activity patterns predicted the PCA content scores. We used MSE to evaluate model performance, comparing it with 1000 permutation model results.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We found that repetition-related differences in medial and lateral parietal cortex predicted image content
significantly above chance level (1000 permutation test). Moreover, these predictions were significantly better for hits (correct recognition) compared to misses (failed recognition) indicating that the presence of content information was directly related to successful recognition. Interestingly, repetition-related differences in occipitotemporal cortex also predicted image content, but the success of these predictions were less dependent on successful behavioral recognition. Collectively, our results indicate that repetition-related enhancements which have consistently been observed in parietal cortex directly integrate information about the content of what is being remembered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient way of the brain for resolving similar memory interference</title>
      <link>/project/color-differentiation/</link>
      <pubDate>Sun, 21 Feb 2021 15:10:03 -0800</pubDate>
      <guid>/project/color-differentiation/</guid>
      <description>&lt;p&gt;The paper is published on Journal of Neuroscience: &lt;a href=&#34;https://www.jneurosci.org/content/41/13/3014.full#sec-27&#34;&gt;https://www.jneurosci.org/content/41/13/3014.full#sec-27&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Given the vast number of memories that humans store, overlap between memories is inevitable. The similarity between memories could cause confusion when trying to retrieve a certain piece of memory. For example, one student in your stats class is called Mario, while another student in your algorithm class is called Wario. These two guys not only have look similar but also tend to dress in a similar style. You may accidentally call Mario, Wario. However, after several weeks of classes, you can definitely tell Mario and Wario apart because your brain develop some strategies to mange the competition in memory.&lt;/p&gt;
&lt;p&gt;Evidence from recent neuroimaging studies hints at the idea that memory representations are distorted as an adaptive response to interference. Namely, several studies have found that, when similar events are encoded into memory, this triggers a targeted exaggeration of differences in patterns of activity. Yet, a critical limitation of these studies is that the feature dimensions along which memories move are underspecified. That is, do changes in neural representations correspond to changes in the information content of memories?&lt;/p&gt;
&lt;p&gt;Here, we tested whether interference between highly similar memories triggers adaptive distortions in memory representations and corresponding behavioral expressions of memories. Our motivating theoretical perspective was that subtle differences between similar memories are prioritized and exaggerated to reduce the potential for interference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;controlled-experiment-design&#34;&gt;Controlled experiment design&lt;/h2&gt;
&lt;p&gt;In order to answer the question, we used color as the memory feature to probe since that color is 1. continuous and 2. can be reported by participants.&lt;/p&gt;
&lt;p&gt;Specially, we used a 2-day procedure in which participants received extensive behavioral training on face-object associations on day 1 and then returned on day 2 for additional behavioral training, followed by an fMRI session, and finally a behavioral color memory test. A critical feature of our design is that we held color similarity between pairmates constant (24 degrees apart), but we included a &lt;strong&gt;competitive&lt;/strong&gt; and &lt;strong&gt;noncompetitive condition&lt;/strong&gt;. In the competitive condition, pairmate images corresponded to the same object category (e.g., two beanbags of slightly different colors). In the noncompetitive condition, pairmates corresponded to distinct object categories (e.g., a pillow and a ball of slightly different colors). Thus, in both conditions, the pairmates were 24 degrees apart in color space; but, for the competitive condition, color was the only feature dimension on which the pairmates differed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%201.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%201.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%202.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%202.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;h3 id=&#34;behavioral-performance-analysis&#34;&gt;Behavioral performance analysis&lt;/h3&gt;
&lt;p&gt;As for the result, first we found that participants exaggerated the color difference between the two similar objects for only for the competitive condition, not for the non-competitive condition. Moreover, the greater memory exaggeration was associated with lower memory interference (indicated by the better associative memory performance). See figures below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%203.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%203.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;neural-imaging-data-fmri-analysis&#34;&gt;Neural imaging data (fMRI) analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Image Data processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After pass the fMRI through the basic preprocessing pipeline, we smoothed the data with a 1.7 mm FWHM Gaussian kernel and high pass filtered at 0.01 Hz to increase signal-to-noise-ratio (&lt;strong&gt;SNR&lt;/strong&gt;). We modeled data with “least-squares separate” method. With this method, each item was estimated in a separate GLM as a separate regressor while all remaining items were modeled together with another regressor. The six movement parameters and framewise displacement were included in each GLM as confound regressors. This resulted in t maps that were used for the multivariate pattern analysis (&lt;strong&gt;MVPA&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%204.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%204.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neural representation of color information during recall&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As predicted, the greater relevance of color information in the competitive condition resulted in stronger representation of color information during recall, despite the fact that participants had not been explicitly oriented to color information in any way by this point of the experiment (the critical behavioral test of color memory occurred after fMRI scanning).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%205.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%205.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neural measures of pairmate similarity predict color memory bias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we found only for the competitive condition, the more dissimilar vIPS activity patterns were when recalling pairmates, the greater the color memory repulsion effect for those pairmates. A mediation analysis performed at the level of individual pairmates also revealed that the relationship between vIPS dissimilarity and associative memory accuracy was significantly mediated by signed color memory distance, consistent with the interpretation that vIPS dissimilarity reflected the degree of color memory repulsion, which in turn was associated with better associative memory accuracy (lower interference).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Here, we show that competition between similar memories triggers biases in their neural representations and corresponding behavioral expressions. Specifically, we demonstrate that subtle, diagnostic differences between events were exaggerated in long-term memory and that this exaggeration reduced interference. Critically, these behavioral expressions of memory distortion were predicted by adaptive, feature-specific changes to memory representations in parietal cortex.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New &lt;a href=&#34;https://twitter.com/hashtag/JNeurosci?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#JNeurosci&lt;/a&gt; research from &lt;a href=&#34;https://twitter.com/_zhaoyufei?ref_src=twsrc%5Etfw&#34;&gt;@_zhaoyufei&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/AChanales?ref_src=twsrc%5Etfw&#34;&gt;@AChanales&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/KuhlLab?ref_src=twsrc%5Etfw&#34;&gt;@KuhlLab&lt;/a&gt; demonstrates that in order to remember similar events, the brain exaggerates the difference between them, resulting in divergent brain activity patterns but better memory performance.&lt;a href=&#34;https://t.co/JTFCrG4d5t&#34;&gt;https://t.co/JTFCrG4d5t&lt;/a&gt; &lt;a href=&#34;https://t.co/HkopLO1rkF&#34;&gt;pic.twitter.com/HkopLO1rkF&lt;/a&gt;&lt;/p&gt;&amp;mdash; SfN Journals (@SfNJournals) &lt;a href=&#34;https://twitter.com/SfNJournals/status/1363911724060000258?ref_src=twsrc%5Etfw&#34;&gt;February 22, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
  </channel>
</rss>
