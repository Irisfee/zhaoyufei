<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yufei Zhao | 赵雨菲</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Yufei Zhao | 赵雨菲</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Yufei Zhao ©2021</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu8daf07698441511167650349ca463977_8870_512x512_fill_lanczos_center_2.png</url>
      <title>Yufei Zhao | 赵雨菲</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building API for Predicting Mando-pop Popularity</title>
      <link>/project/hitsong/</link>
      <pubDate>Sun, 22 Aug 2021 00:00:52 -0700</pubDate>
      <guid>/project/hitsong/</guid>
      <description>&lt;p&gt;Desciption is coming soon! You can check out the two github repo now!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes for Machine Learning Data Lifecycle in Production (MLOps2) on Coursera/Deeplearning.ai</title>
      <link>/post/mlops2/</link>
      <pubDate>Wed, 30 Jun 2021 14:13:39 -0700</pubDate>
      <guid>/post/mlops2/</guid>
      <description>&lt;h1 id=&#34;week1&#34;&gt;Week1&lt;/h1&gt;
&lt;h2 id=&#34;introduction-to-machine-learning-engineering-in-production&#34;&gt;Introduction to Machine Learning Engineering in Production&lt;/h2&gt;
&lt;h3 id=&#34;managing-the-entire-life-cycle-of-data&#34;&gt;Managing the entire life cycle of data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Labeling&lt;/li&gt;
&lt;li&gt;Feature space coverage&lt;/li&gt;
&lt;li&gt;Minimal dimensionality&lt;/li&gt;
&lt;li&gt;Maximum predictive data&lt;/li&gt;
&lt;li&gt;Fairness&lt;/li&gt;
&lt;li&gt;Rare conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;modern-software-development&#34;&gt;Modern software development&lt;/h3&gt;
&lt;p&gt;Accounts for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scalability&lt;/li&gt;
&lt;li&gt;Extensibility&lt;/li&gt;
&lt;li&gt;Configureation&lt;/li&gt;
&lt;li&gt;Consistency &amp;amp; reproducibility&lt;/li&gt;
&lt;li&gt;Safety &amp;amp; security&lt;/li&gt;
&lt;li&gt;Modularity&lt;/li&gt;
&lt;li&gt;Testability&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;li&gt;Best practice&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;production-machine-learning-system&#34;&gt;Production machine learning system&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;challenges-in-production-grade-ml&#34;&gt;Challenges in production grade ML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Build integrated ML systems&lt;/li&gt;
&lt;li&gt;Continuously operate it in production&lt;/li&gt;
&lt;li&gt;Handle continuously changing data&lt;/li&gt;
&lt;li&gt;Optimize compute resource costs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;production-ml-infrastructure&#34;&gt;Production ML infrastructure&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%201.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%201.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;directed-acyclic-graphs&#34;&gt;Directed acyclic graphs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A directed acyclic graph (DAG) is a directed graph that has no cycles&lt;/li&gt;
&lt;li&gt;ML pipeline workflows are usually DAGs.&lt;/li&gt;
&lt;li&gt;DAGs define the sequencing of the tasks to be performed, based on their relationships and dependencies.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%202.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%202.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pipeline-orchestration-frameworks&#34;&gt;Pipeline orchestration frameworks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%203.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%203.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Responsible for scheduling the various components in an ML pipeline DAG dependencies&lt;/li&gt;
&lt;li&gt;Help with pipeline automation&lt;/li&gt;
&lt;li&gt;Examples: Airflow, Argo, Celery, Luigi, Kubeflow&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tensorflow-extended-tfx&#34;&gt;TensorFlow Extended (TFX)&lt;/h3&gt;
&lt;p&gt;End-to-end platform for deploying production ML pipelines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%204.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%204.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Sequence of components that are designed for scalable, high-performance machine learning tasks.&lt;/p&gt;
&lt;h3 id=&#34;tfx-production-components&#34;&gt;TFX production components&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%205.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%205.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;tfx-hello-world&#34;&gt;TFX Hello World&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%206.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%206.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Production ML pipelines: automating, monitoring, and maintaining end-to-end processes&lt;/li&gt;
&lt;li&gt;Production ML is much more than just ML code
&lt;ul&gt;
&lt;li&gt;ML development + software development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TFX is an open-source end-to-end ML platform&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;collecting-data&#34;&gt;Collecting Data&lt;/h2&gt;
&lt;h3 id=&#34;ml-data-is-a-first-class-citizen&#34;&gt;ML: Data is a first class citizen&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Software 1.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explicit instructions to the computer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Software 2.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specify some goal on the behavior of a program&lt;/li&gt;
&lt;li&gt;Find solution using optimization techniques&lt;/li&gt;
&lt;li&gt;Good data is key for success&lt;/li&gt;
&lt;li&gt;Code in Software = Data in ML&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%207.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%207.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;everything-starts-with-data&#34;&gt;Everything starts with data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Models aren&amp;rsquo;t magic&lt;/li&gt;
&lt;li&gt;Meaningful data:
&lt;ul&gt;
&lt;li&gt;maximize predictive content&lt;/li&gt;
&lt;li&gt;remove non-informative data&lt;/li&gt;
&lt;li&gt;feature space coverage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-pipeline&#34;&gt;Data pipeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%208.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%208.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-collection-and-monitoring&#34;&gt;Data collection and monitoring&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%209.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%209.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-1&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Understand users, translate user needs into data problems&lt;/li&gt;
&lt;li&gt;Ensure data coverage and high predictive signal&lt;/li&gt;
&lt;li&gt;Source, store and monitor quality data responsibly&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-application-suggesting-runs&#34;&gt;Example application: suggesting runs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2010.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2010.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-considerations&#34;&gt;Key considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data availability and collection
&lt;ul&gt;
&lt;li&gt;What kind of /how much data is available?&lt;/li&gt;
&lt;li&gt;how often does the new data come in?&lt;/li&gt;
&lt;li&gt;Is it annotated?
&lt;ul&gt;
&lt;li&gt;If not, how hard/expensive is it to get it labeled?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Translate user needs into data needs
&lt;ul&gt;
&lt;li&gt;Data needed&lt;/li&gt;
&lt;li&gt;Features needed&lt;/li&gt;
&lt;li&gt;Labels needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-dataset&#34;&gt;Example dataset&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2011.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2011.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;get-to-know-your-data&#34;&gt;Get to know your data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Identify data sources&lt;/li&gt;
&lt;li&gt;check if they are refreshed&lt;/li&gt;
&lt;li&gt;Consistency for values, units, and types&lt;/li&gt;
&lt;li&gt;Monitor outliers and errors&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataset-issues&#34;&gt;Dataset issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Inconsistent formatting
&lt;ul&gt;
&lt;li&gt;Is zero &amp;lsquo;&amp;lsquo;0&amp;quot;, &amp;ldquo;0.0&amp;rdquo;, or an indicator of a missing measurement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compounding errors from other ML models&lt;/li&gt;
&lt;li&gt;Monitor data sources for system issues and outages&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;measure-data-effectiveness&#34;&gt;Measure data effectiveness&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intuition about data value can misleading
&lt;ul&gt;
&lt;li&gt;Which features have predictive value and which ones do not?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Feature engineering helps to maximize the predictive signals&lt;/li&gt;
&lt;li&gt;Feature selection helps to measure the predictive signals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;translate-user-needs-into-data-needs&#34;&gt;Translate user needs into data needs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2012.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2012.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2013.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2013.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2014.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2014.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-2&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Understand your user, translate their needs into data problems
&lt;ul&gt;
&lt;li&gt;What kind of / how much data is available&lt;/li&gt;
&lt;li&gt;What are the details and issues of your data&lt;/li&gt;
&lt;li&gt;What are your predictive features&lt;/li&gt;
&lt;li&gt;What are the labels you are tracking&lt;/li&gt;
&lt;li&gt;What are your metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;avoiding-problematic-biases-in-datasets&#34;&gt;Avoiding problematic biases in datasets&lt;/h3&gt;
&lt;h3 id=&#34;source-data-responsibly&#34;&gt;Source data responsibly&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2015.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2015.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-security-and-privacy&#34;&gt;Data security and privacy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data collection and management isn&amp;rsquo;t just about your model
&lt;ul&gt;
&lt;li&gt;Give user control of what data can be collected&lt;/li&gt;
&lt;li&gt;Is there a risk of inadvertently revealing user data?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compliance with regulations and policies (e.g. GDPR)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;users-privacy&#34;&gt;Users privacy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Protect personally identifiable information
&lt;ul&gt;
&lt;li&gt;Aggregation - replace unique values with summary value&lt;/li&gt;
&lt;li&gt;Redaction - remove some data to create less complete picture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-ml-systems-can-fail-users&#34;&gt;How ML systems can fail users&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2016.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2016.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Representational Harm&lt;/li&gt;
&lt;li&gt;Opportunity denial&lt;/li&gt;
&lt;li&gt;Disproportionate product failure&lt;/li&gt;
&lt;li&gt;Harm by disadvantage&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;commit-to-fairness&#34;&gt;Commit to fairness&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Make sure your models are fair
&lt;ul&gt;
&lt;li&gt;Group fairness, equal accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bias in human labeled and/or collected data&lt;/li&gt;
&lt;li&gt;ML models can amplify biases&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reducing-bias-design-fair-labeling-systems&#34;&gt;Reducing bias: design fair labeling systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Accurate labels are necessary for supervised learning&lt;/li&gt;
&lt;li&gt;Labeling can be done by:
&lt;ul&gt;
&lt;li&gt;Automation (logging or weak supervision)&lt;/li&gt;
&lt;li&gt;Humans (aka &amp;ldquo;raters&amp;rdquo;, often semi-supervised)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;types-of-human-raters&#34;&gt;Types of human raters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2017.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2017.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-3&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ensure raters pool diversity&lt;/li&gt;
&lt;li&gt;Investigate rater context and incentives&lt;/li&gt;
&lt;li&gt;Evaluate rater tools&lt;/li&gt;
&lt;li&gt;Manage cost&lt;/li&gt;
&lt;li&gt;Determine freshness requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;labeling-data&#34;&gt;Labeling Data&lt;/h2&gt;
&lt;h3 id=&#34;case-study-taking-action&#34;&gt;Case study: taking action&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How to detect problems early on?&lt;/li&gt;
&lt;li&gt;What are the possible causes?&lt;/li&gt;
&lt;li&gt;What can be done to solve these?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-causes-problesm&#34;&gt;What causes problesm?&lt;/h3&gt;
&lt;p&gt;kinds of problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slow: drift&lt;/li&gt;
&lt;li&gt;Fast: bad sensor, bad software update&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gradual-problems&#34;&gt;Gradual problems&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2018.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2018.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sudden-problems&#34;&gt;Sudden problems&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2019.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2019.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-understand-the-model&#34;&gt;Why &amp;ldquo;understand&amp;rdquo; the model?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mispredictions do not have uniform cost to your business&lt;/li&gt;
&lt;li&gt;The data you have is rarely the data you wish you had&lt;/li&gt;
&lt;li&gt;Model objective is nearly always a proxy for your business objectives&lt;/li&gt;
&lt;li&gt;Some percentage of your customers may have a bad experience&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;detecting-problems-with-deployed-models&#34;&gt;Detecting problems with deployed models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data and scope changes&lt;/li&gt;
&lt;li&gt;Monitor models and validate data to find problems early&lt;/li&gt;
&lt;li&gt;Changing ground truth: label new training data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;easy-problems&#34;&gt;Easy problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ground truth changes slowly (months, years)&lt;/li&gt;
&lt;li&gt;Model retraining driven by:
&lt;ul&gt;
&lt;li&gt;Model improvements, better data&lt;/li&gt;
&lt;li&gt;Changes in software and/or systems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeling
&lt;ul&gt;
&lt;li&gt;Curated datasets&lt;/li&gt;
&lt;li&gt;Crowed-based&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;harder-problems&#34;&gt;Harder problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ground truth changes faster (weeks)&lt;/li&gt;
&lt;li&gt;Model retraining driven by:
&lt;ul&gt;
&lt;li&gt;Declining model performance&lt;/li&gt;
&lt;li&gt;Model improvements, better data&lt;/li&gt;
&lt;li&gt;Changes in software and/or system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeling
&lt;ul&gt;
&lt;li&gt;Direct feedback&lt;/li&gt;
&lt;li&gt;Crowd-based&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;really-hard-problems&#34;&gt;Really hard problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ground truth changes very fast (days, hours, min)&lt;/li&gt;
&lt;li&gt;Model retraining driven by:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Declining model performance&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Model improvements, better data&lt;/li&gt;
&lt;li&gt;Changes in software and/or system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeling
&lt;ul&gt;
&lt;li&gt;Direct feedback&lt;/li&gt;
&lt;li&gt;Weak supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-points-4&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model performance decays over time
&lt;ul&gt;
&lt;li&gt;Data and concept drift&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model retraining helps to improve performance
&lt;ul&gt;
&lt;li&gt;Data labeling for changing ground truth and scarce labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-labeling&#34;&gt;Data labeling&lt;/h3&gt;
&lt;p&gt;Variety of Methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Process Feedback (Direct labeling)&lt;/li&gt;
&lt;li&gt;Human labeling&lt;/li&gt;
&lt;li&gt;Semi-supervised labeling&lt;/li&gt;
&lt;li&gt;Active learning&lt;/li&gt;
&lt;li&gt;Weak supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2020.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2020.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-is-labeling-important-in-production-ml&#34;&gt;Why is labeling important in production ML?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;using business/organization available data&lt;/li&gt;
&lt;li&gt;Frequent model retraining&lt;/li&gt;
&lt;li&gt;Labeling ongoing and critical process&lt;/li&gt;
&lt;li&gt;Creating a training datasets requires labels&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;direct-labeling-continuous-creation-of-training-dataset&#34;&gt;Direct labeling: continuous creation of training dataset&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2021.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2021.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;process-feedback---advantages&#34;&gt;Process feedback - advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Training dataset continuous creation&lt;/li&gt;
&lt;li&gt;Labels evolve quickly&lt;/li&gt;
&lt;li&gt;Captures strong label signals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;process-feedback--disadantages&#34;&gt;Process feedback -disadantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hindered by inherent nature of the problem&lt;/li&gt;
&lt;li&gt;Failure to capture ground truth&lt;/li&gt;
&lt;li&gt;Largely bespoke design&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;process-feedback--open-source-log-analysis-tools&#34;&gt;Process feedback- Open-source log analysis tools&lt;/h3&gt;
&lt;p&gt;Logstash: free and open source data processing pipeline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ingests data from a multitude of sources&lt;/li&gt;
&lt;li&gt;Transforms it&lt;/li&gt;
&lt;li&gt;Sends it to your favorite &amp;ldquo;stash&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fluentd&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open source data collector&lt;/li&gt;
&lt;li&gt;Unify the data collection and consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;process-feedback-cloud-log-analytics&#34;&gt;Process feedback-Cloud log analytics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2022.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2022.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;human-labeling&#34;&gt;Human labeling&lt;/h3&gt;
&lt;p&gt;People (&amp;lsquo;raters&amp;rsquo;) to examine data and assign labels manually&lt;/p&gt;
&lt;p&gt;Methodology:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlabeled data is collected&lt;/li&gt;
&lt;li&gt;Human (&amp;lsquo;raters&amp;rsquo;) are recruited&lt;/li&gt;
&lt;li&gt;Instructions to guide raters are created&lt;/li&gt;
&lt;li&gt;Data is divided and assigned to raters&lt;/li&gt;
&lt;li&gt;Labels are collected and conflicts resolved&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;human-labeling---advantages&#34;&gt;Human labeling - advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;More labels&lt;/li&gt;
&lt;li&gt;Pure supervised learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;human-labeling---disadvantages&#34;&gt;Human labeling - disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Quality consistency: many datasets difficult for human labeling&lt;/li&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Expensive&lt;/li&gt;
&lt;li&gt;Small dataset curation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;validating-data&#34;&gt;Validating Data&lt;/h2&gt;
&lt;h3 id=&#34;drift-and-skew&#34;&gt;Drift and skew&lt;/h3&gt;
&lt;p&gt;Drift: changes in data over time, such as data collected once a day&lt;/p&gt;
&lt;p&gt;Skew: Difference between two static versions, or different sources, such as training set and serving set&lt;/p&gt;
&lt;h3 id=&#34;typical-ml-pipeline&#34;&gt;Typical ML pipeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2023.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2023.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;model-decay-data-drift&#34;&gt;Model decay: data drift&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2024.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2024.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;performance-decay-concept-drift&#34;&gt;Performance decay: concept drift&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2025.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2025.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;detecting-data-issues&#34;&gt;Detecting data issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Detecting schema skew
&lt;ul&gt;
&lt;li&gt;Training and serving data do not conform to be the same schema&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Detecting distribution skew
&lt;ul&gt;
&lt;li&gt;Dataset shift → covariate or concept shift&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Requires continuous evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;detecting-distribution-skew&#34;&gt;Detecting distribution skew&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2026.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2026.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;skew-detection-workflow&#34;&gt;Skew detection workflow&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2027.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2027.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;tensorflow-data-validation-tfdv&#34;&gt;TensorFlow Data Validation (TFDV)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Understand, validate, and monitor ML data at scale&lt;/li&gt;
&lt;li&gt;Used to analyze and validate petabytes of data at google every day&lt;/li&gt;
&lt;li&gt;Proven track record in helping TFX users maintain the health of their ML pipelines&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tfdv-capabilities&#34;&gt;TFDV capabilities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generates data statistics and browser visualizations&lt;/li&gt;
&lt;li&gt;Infers the data schema&lt;/li&gt;
&lt;li&gt;Performs validity checks against schema&lt;/li&gt;
&lt;li&gt;Detects training/serving skew&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;skew--detection---tfdv&#34;&gt;Skew  detection - TFDV&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2028.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2028.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;skew--tfdv&#34;&gt;Skew -TFDV&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supported for categorical features&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Expressed in terms of L-infinity distance (Chebyshev Distance):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2029.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2029.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2030.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2030.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set a threshold to receive warnings&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schema-skew&#34;&gt;Schema skew&lt;/h3&gt;
&lt;p&gt;Serving and training data don&amp;rsquo;t conform to same schema: int ≠ float&lt;/p&gt;
&lt;h3 id=&#34;feature-skew&#34;&gt;Feature skew&lt;/h3&gt;
&lt;p&gt;Training feature values are different than the serving feature values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature values are modified between training and serving time&lt;/li&gt;
&lt;li&gt;Transformation applied only in one of the two instances&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distribution-skew&#34;&gt;Distribution skew&lt;/h3&gt;
&lt;p&gt;Distribution of serving and training dataset is significantly different:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faulty sampling method during training&lt;/li&gt;
&lt;li&gt;Different data sources for training and serving data&lt;/li&gt;
&lt;li&gt;Trend, seasonality, changes in data over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-points-5&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TFDV: descriptive statistics at scale with the embedded facets visualizations&lt;/li&gt;
&lt;li&gt;It provides insight into:
&lt;ul&gt;
&lt;li&gt;What are the underlying statistics of your data&lt;/li&gt;
&lt;li&gt;How does your training, evaluation, and serving dataset statistics compare&lt;/li&gt;
&lt;li&gt;How can you detect and fix data anomalies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;readings&#34;&gt;Readings:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cd.foundation/blog/2020/02/11/announcing-the-cd-foundation-mlops-sig/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLops&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@karpathy/software-2-0-a64152b37c35&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data 1st class citizen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pair.withgoogle.com/chapter/data-collection/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runners app&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/guides/rules-of-ml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rules of ML&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bias in datasets&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/logstash&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Logstash&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.fluentd.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fluentd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Logging&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/elasticsearch-service/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS ElasticSearch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/monitor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure Monitor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TFDV&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chebyshev_distance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chebyshev distance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Konstantinos, Katsiapis, Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., … Li, Z. (2020). Towards ML Engineering: A brief history of TensorFlow Extended (TFX). &lt;a href=&#34;http://arxiv.org/abs/2010.02013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/2010.02013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paleyes, A., Urma, R.-G., &amp;amp; Lawrence, N. D. (2020). Challenges in deploying machine learning: A survey of case studies. &lt;a href=&#34;http://arxiv.org/abs/2011.09926&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/2011.09926&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ML code fraction:&lt;/p&gt;
&lt;p&gt;Sculley, D., Holt, G., Golovin, D., Davydov, E., &amp;amp; Phillips, T. (n.d.). Hidden technical debt in machine learning systems. Retrieved April 28, 2021, from Nips.cc &lt;a href=&#34;https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;week-2&#34;&gt;Week 2&lt;/h1&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;h3 id=&#34;squeezing-the-most-out-of-data&#34;&gt;Squeezing the most out of data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Making data useful before training a model&lt;/li&gt;
&lt;li&gt;Representing data in forms that help models learn&lt;/li&gt;
&lt;li&gt;Increasing predictive quality&lt;/li&gt;
&lt;li&gt;Reducing dimensionality with feature engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;art-of-feature-engineering&#34;&gt;Art of feature engineering&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2031.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2031.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;typical-ml-pipeline-1&#34;&gt;Typical ML pipeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2032.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2032.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-6&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineering can be difficult and time consuming, but also very important to success&lt;/li&gt;
&lt;li&gt;Squeezing the most out of data through feature engineering enables models to learn better&lt;/li&gt;
&lt;li&gt;Concentrating predictive information in fewer features enables more efficient use of compute resources&lt;/li&gt;
&lt;li&gt;Feature engineering during training must also be applied correctly during serving&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-preprocessing-operations&#34;&gt;Main preprocessing operations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2033.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2033.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mapping-raw-data-into-features&#34;&gt;Mapping raw data into features&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2034.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2034.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;mapping-categorical-values&#34;&gt;Mapping categorical values&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2035.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2035.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;categorical-vocabulary&#34;&gt;Categorical vocabulary&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2036.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2036.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;empirical-knowledge-of-data&#34;&gt;Empirical knowledge of data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text: stemming, lemmatization, TF-IDF, n-grams, embedding lookup&lt;/li&gt;
&lt;li&gt;Images: clipping, resizing, cropping, blur, canny filters, Sobel filter, photometric distortions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-points-7&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data preprocessing: transforms raw data into a clean and training-ready dataset&lt;/li&gt;
&lt;li&gt;Feature engineering maps:
&lt;ul&gt;
&lt;li&gt;Raw data into feature vectors&lt;/li&gt;
&lt;li&gt;Integer values to floating-point values&lt;/li&gt;
&lt;li&gt;Normalizes numerical values&lt;/li&gt;
&lt;li&gt;Strings and categorical values to vectors of numeric values&lt;/li&gt;
&lt;li&gt;Data from one space into a different space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-engineering-techniques&#34;&gt;Feature engineering techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Numerical range: scaling, normalizing, standardizing&lt;/li&gt;
&lt;li&gt;Grouping: bucketizing, bag of words&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scaling&#34;&gt;Scaling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Converts values from their natural range into a prescribed range
&lt;ul&gt;
&lt;li&gt;E.g. grayscale image pixel intensity scale is [0,255] usually rescaled to [-1,1]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Benefits
&lt;ul&gt;
&lt;li&gt;Helps neural nets converge faster&lt;/li&gt;
&lt;li&gt;Do away with NaN errors during training&lt;/li&gt;
&lt;li&gt;For each feature, the model learns the right weights&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;normalization&#34;&gt;Normalization&lt;/h3&gt;
&lt;p&gt;If you know the data is not gaussian usually good to do.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2037.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2037.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2038.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2038.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;standardization-z-score&#34;&gt;Standardization (z-score)&lt;/h3&gt;
&lt;p&gt;if your data is normal distribution; try both standardization and normalization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Z-score relates the number of standard deviations away from the mean&lt;/li&gt;
&lt;li&gt;e.g.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2039.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2039.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2040.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2040.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;bucketizingbinning&#34;&gt;Bucketizing/Binning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2041.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2041.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;binning-with-facets&#34;&gt;Binning with facets&lt;/h3&gt;
&lt;h3 id=&#34;other-techniques&#34;&gt;Other techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dimensionality reduction in embeddings
&lt;ul&gt;
&lt;li&gt;PCA: principal component analysis&lt;/li&gt;
&lt;li&gt;t-SNE: t-Distributed stochastic neighbor embedding (t-SNE)&lt;/li&gt;
&lt;li&gt;UMAP: uniform manifold approximation and projection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Feature crossing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tensorflow-embedding-projector&#34;&gt;TensorFlow embedding projector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intuitive exploration of high-dimensional data&lt;/li&gt;
&lt;li&gt;Visualize and analyze&lt;/li&gt;
&lt;li&gt;Techniques: PCA,t-SNE, UMAP, custom linear projections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2042.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2042.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-8&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineering:
&lt;ul&gt;
&lt;li&gt;Prepares, tunes, transforms, extracts, and constructs features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Feature engineering is key for model refinement&lt;/li&gt;
&lt;li&gt;Feature engineering helps with ML analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-crosses&#34;&gt;Feature crosses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Combines multiple features together into a new feature&lt;/li&gt;
&lt;li&gt;Encodes nonlinearity in the feature space, or encodes the same information in fewer features&lt;/li&gt;
&lt;li&gt;Create many different kinds of feature crosses
&lt;ul&gt;
&lt;li&gt;[A*B]: multiplying the values of two features&lt;/li&gt;
&lt;li&gt;[A&lt;em&gt;B&lt;/em&gt;C&lt;em&gt;D&lt;/em&gt;E]: multiplying the values of 5 features&lt;/li&gt;
&lt;li&gt;[Day of week, hour]⇒[Hour of week]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;encoding-features&#34;&gt;Encoding features&lt;/h3&gt;
&lt;p&gt;can&amp;rsquo;t be separated by a line&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2043.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2043.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-transformation-at-scale&#34;&gt;Feature Transformation at Scale&lt;/h2&gt;
&lt;h3 id=&#34;preprocessing-data-at-scale&#34;&gt;Preprocessing data at scale&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Real-world models: terabytes of data&lt;/li&gt;
&lt;li&gt;Large-scale data processing frameworks&lt;/li&gt;
&lt;li&gt;Consistent transforms between training and serving&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inconsistencies-in-feature-engineering&#34;&gt;Inconsistencies in feature engineering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Training and serving code paths are different&lt;/li&gt;
&lt;li&gt;Diverse deployments scenarios
&lt;ul&gt;
&lt;li&gt;Mobile (TF lite)&lt;/li&gt;
&lt;li&gt;Server (TF serving)&lt;/li&gt;
&lt;li&gt;Web (TF JS)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Risks of introducing training-serving skews&lt;/li&gt;
&lt;li&gt;Skews will lower the performance of your serving model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preprocessing-granularity&#34;&gt;Preprocessing granularity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2044.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2044.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;when-do-you-transform&#34;&gt;When do you transform?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2045.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2045.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-about-within-a-model&#34;&gt;How about &amp;lsquo;within&amp;rsquo; a model?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2046.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2046.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-transform-per-batch&#34;&gt;Why transform per batch?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For example, normalizing features by their average&lt;/li&gt;
&lt;li&gt;Access to a single batch of data, not the full dataset&lt;/li&gt;
&lt;li&gt;Ways to normalize per batch
&lt;ul&gt;
&lt;li&gt;Normalize by average within a batch&lt;/li&gt;
&lt;li&gt;Precompute average and reuse it during normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimizing-instance-level-transformations&#34;&gt;Optimizing instance-level transformations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Indirectly affect training efficiency&lt;/li&gt;
&lt;li&gt;Typically accelerators sit idle while the CPUs transform&lt;/li&gt;
&lt;li&gt;Solution: prefetching transforms for better accelerator efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summarizing-the-challenges&#34;&gt;Summarizing the challenges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Balancing predictive performance&lt;/li&gt;
&lt;li&gt;Full-pass transformations on training data&lt;/li&gt;
&lt;li&gt;Optimizing instance-level transformations for better training efficiency (GPUs, TPUs,&amp;hellip;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;enter-tftransform&#34;&gt;Enter tf.Transform&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2047.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2047.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inside-tf-extended&#34;&gt;Inside TF Extended&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2048.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2048.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;rftransform-layout&#34;&gt;rf.Transform layout&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2049.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2049.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;tftransform-going-deeper&#34;&gt;tf.Transform: going deeper&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2050.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2050.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;rftransform-analyzers&#34;&gt;rf.Transform Analyzers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2051.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2051.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-transform-applies-feature-transformations&#34;&gt;How transform applies feature transformations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2052.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2052.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;benefits-of-using-tftransform&#34;&gt;Benefits of using tf.Transform&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Emitted tf.Graph holds all necessary constants and transformations&lt;/li&gt;
&lt;li&gt;Focus on data preprocessing only at training time&lt;/li&gt;
&lt;li&gt;Works in-line during both training and serving&lt;/li&gt;
&lt;li&gt;No need for preprocessing code at serving time&lt;/li&gt;
&lt;li&gt;Consistently applied transformations irrespective of deployment platform&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analyzers-framework&#34;&gt;Analyzers framework&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2053.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2053.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;tftransform-preprocessing_fn&#34;&gt;tf.Transform preprocessing_fn&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2054.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2054.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;commonly-used-imports&#34;&gt;Commonly used imports&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2055.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2055.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;hello-world-with-tftransform&#34;&gt;Hello world with tf.Transform&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2056.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2056.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collect-raw-samples-data&#34;&gt;Collect raw samples (data)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2057.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2057.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inspect-data-and-prepare-metadata-data&#34;&gt;Inspect data and prepare metadata (data)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2058.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2058.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;preprocessing-data-transform&#34;&gt;Preprocessing data (transform)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2059.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2059.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2060.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2060.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;tensors-intensors-out&#34;&gt;Tensors in&amp;hellip;tensors out&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2061.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2061.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;running-the-pipeline&#34;&gt;Running the pipeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2062.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2062.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2063.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2063.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-selection&#34;&gt;Feature Selection&lt;/h2&gt;
&lt;h3 id=&#34;feature-space&#34;&gt;Feature space&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;N dimensional space defined by your N features&lt;/li&gt;
&lt;li&gt;Not including the target label&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-space-coverage&#34;&gt;Feature space coverage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Train/eval datasets representative of the serving dataset
&lt;ul&gt;
&lt;li&gt;same numerical ranges&lt;/li&gt;
&lt;li&gt;same classes&lt;/li&gt;
&lt;li&gt;similar characteristics for image data&lt;/li&gt;
&lt;li&gt;similar vocabulary, syntax, and semantics for NLP problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ensure-feature-space-voerage&#34;&gt;Ensure feature space voerage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data affected by: seasonality, trend, drift&lt;/li&gt;
&lt;li&gt;Serving data: new values in features and labels&lt;/li&gt;
&lt;li&gt;Continuous monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-selection-1&#34;&gt;Feature selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Identify features that best represent the relationship&lt;/li&gt;
&lt;li&gt;Remove features that don&amp;rsquo;t influence the outcome&lt;/li&gt;
&lt;li&gt;Reduce the size of the feature space&lt;/li&gt;
&lt;li&gt;Reduce the resource requirements and model complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-is-feature-selection-needed&#34;&gt;Why is feature selection needed?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reduce storage and I/O requirements&lt;/li&gt;
&lt;li&gt;Minimize training and inference costs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-selection-methods&#34;&gt;Feature selection methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unsupervised
&lt;ul&gt;
&lt;li&gt;Features-target variable relationship not considered&lt;/li&gt;
&lt;li&gt;Removes redundant features (correlation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supervised
&lt;ul&gt;
&lt;li&gt;Uses features-target variable relationsihp&lt;/li&gt;
&lt;li&gt;Selects those contributing the most&lt;/li&gt;
&lt;li&gt;Filter methods/Wrapper Methods/Embedded Methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;filter-methods&#34;&gt;Filter methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Correlation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Univariate feature selection (for efficiency)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Popular filter methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pearson correlation: between features, and between the features and the label&lt;/li&gt;
&lt;li&gt;Univariate feature selection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2064.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2064.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correlation-matrix&#34;&gt;Correlation matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shows how features are related
&lt;ul&gt;
&lt;li&gt;To each other (bad)&lt;/li&gt;
&lt;li&gt;And with target variable (good)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Falls in the range [-1, 1]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-comparison-statistical-tests&#34;&gt;Feature comparison statistical tests&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pearson&amp;rsquo;s correlation: linear relationships&lt;/li&gt;
&lt;li&gt;Kendall Tau Rank correlation coefficient: monotonic relationships and small sample size&lt;/li&gt;
&lt;li&gt;Spearman&amp;rsquo;s Rank Correlation Coefficient: monotonic relationships&lt;/li&gt;
&lt;li&gt;Mutual information/F-test/Chi-squared test&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;determine-correlation&#34;&gt;Determine correlation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2065.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2065.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;selecting-features&#34;&gt;Selecting features&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2066.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2066.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;performance-table&#34;&gt;Performance table&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2067.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2067.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;univariate-feature-selection-in-sklearn&#34;&gt;Univariate feature selection in sklearn&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2068.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2068.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;wrapper-methods&#34;&gt;Wrapper methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Forward elimination&lt;/li&gt;
&lt;li&gt;backward elimination&lt;/li&gt;
&lt;li&gt;recursive feature elimination&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2069.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2069.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;forward-selection&#34;&gt;Forward selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Iterative, greedy method&lt;/li&gt;
&lt;li&gt;Starts with 1 feature&lt;/li&gt;
&lt;li&gt;Evaluate model performance when adding each of the additional features, one at a time&lt;/li&gt;
&lt;li&gt;Add next feature that gives the best performance&lt;/li&gt;
&lt;li&gt;Repeat until there is no improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-selection&#34;&gt;Backward selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Start with all features&lt;/li&gt;
&lt;li&gt;Evaluate model performance when removing each of the included features, one at a time&lt;/li&gt;
&lt;li&gt;Remove next feature that gives the best performance&lt;/li&gt;
&lt;li&gt;Repeat until there is no improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recursive-feature-elimination-rfe&#34;&gt;Recursive feature elimination (RFE)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Select a model to use for evaluating feature importance&lt;/li&gt;
&lt;li&gt;Select the desired number of features&lt;/li&gt;
&lt;li&gt;Fit the model&lt;/li&gt;
&lt;li&gt;Rank features by importance&lt;/li&gt;
&lt;li&gt;Discard least important features&lt;/li&gt;
&lt;li&gt;Repeat until the desired number of features remains&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;embedded-methods&#34;&gt;Embedded methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L1 regularization&lt;/li&gt;
&lt;li&gt;Feature importance&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-importance&#34;&gt;Feature importance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Assigns scores for each feature in data&lt;/li&gt;
&lt;li&gt;Discard features scored lower by feature importance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;readings-1&#34;&gt;Readings&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/representation/feature-engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mapping raw data into feature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.notion.so/3ce75d036e924c70ab7e47f534ec40fc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feature engineering techniques&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pair-code.github.io/facets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facets&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://projector.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embedding projector&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/feature-crosses/encoding-nonlinearity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Encoding features&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TFX:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tfx/guide#tfx_pipelines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/tfx/guide#tfx_pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ai.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/breast&amp;#43;cancer&amp;#43;wisconsin&amp;#43;%28diagnostic%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Breast Cancer Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;week-3&#34;&gt;Week 3&lt;/h1&gt;
&lt;h2 id=&#34;data-journey-and-data-storage&#34;&gt;Data Journey and Data Storage&lt;/h2&gt;
&lt;h3 id=&#34;the-data-journey&#34;&gt;The data journey&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Raw features and labels&lt;/li&gt;
&lt;li&gt;Input-output map&lt;/li&gt;
&lt;li&gt;ML model to learn mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-transformation&#34;&gt;Data transformation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data transforms as it flows through the process&lt;/li&gt;
&lt;li&gt;Interpreting model results requires understanding data transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;artifacts-and-the-ml-pipeline&#34;&gt;Artifacts and the ML pipeline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Artifacts are created as the components of the ML pipeline execute&lt;/li&gt;
&lt;li&gt;Artifacts include all of the data and objects which are produced by the pipeline components&lt;/li&gt;
&lt;li&gt;This includes the data, in different stages of transformation, the schema, the model itself, metrics, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-provenance-and-lineage&#34;&gt;Data provenance and lineage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The chain of transformations that led to the creation of a particular artifact&lt;/li&gt;
&lt;li&gt;Important for debugging and reproducibility&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-provenance-why-it-matters&#34;&gt;Data provenance: why it matters&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Helps with debugging and understanding the ML pipeline:
&lt;ul&gt;
&lt;li&gt;Inspect artifacts at each point in the training process&lt;/li&gt;
&lt;li&gt;Trace back through a training run&lt;/li&gt;
&lt;li&gt;Compare training runs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-lineage-data-protection-regulation&#34;&gt;Data lineage: data protection regulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Organizations must closely track and organize personal dta&lt;/li&gt;
&lt;li&gt;Data lineage is extremely important for regulatory compliance&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-provenance-interpreting-results&#34;&gt;Data provenance: interpreting results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data transformations sequence leading to predictions&lt;/li&gt;
&lt;li&gt;Understanding the model as it evolves through runs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-versioning&#34;&gt;Data versioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data pipeline management is a major challenge&lt;/li&gt;
&lt;li&gt;machine learning requires reproducibility&lt;/li&gt;
&lt;li&gt;Code versioning: github&amp;hellip;&lt;/li&gt;
&lt;li&gt;Environment versioning: docker, terraform, and similar&lt;/li&gt;
&lt;li&gt;Data versioning:
&lt;ul&gt;
&lt;li&gt;Version control of datasets&lt;/li&gt;
&lt;li&gt;examples: DVC, Git-LFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;metadata-tracking-artifacts-and-pipeline-changes&#34;&gt;Metadata: tracking artifacts and pipeline changes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2070.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2070.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;metadata-tfx-component-architecture&#34;&gt;Metadata: TFX component architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Driver: supplies required metadata to executor&lt;/li&gt;
&lt;li&gt;Executor: place to code the functionality of component&lt;/li&gt;
&lt;li&gt;Publisher: stores result into metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2071.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2071.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ml-metadata-library&#34;&gt;ML metadata library&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tracks metadata flowing between components in pipeline&lt;/li&gt;
&lt;li&gt;Supports multiple storage backends&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-metadata-terminology&#34;&gt;ML metadata terminology&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2072.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2072.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;metadata-stored&#34;&gt;Metadata stored&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2073.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2073.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inside-metadatastore&#34;&gt;Inside metadatastore&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2074.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2074.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-benefits-of-ml-metadata&#34;&gt;Other benefits of ML Metadata&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Produce DAG of pipelines&lt;/li&gt;
&lt;li&gt;Verify the inputs used in a nexecution&lt;/li&gt;
&lt;li&gt;List all artifacts&lt;/li&gt;
&lt;li&gt;Compare artifacts&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-metadata-storage-backend&#34;&gt;ML Metadata storage backend&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ML metadata registers metadata in a database called metadata store&lt;/li&gt;
&lt;li&gt;APIs to record and retrieve metadata to and from the storage backend:
&lt;ul&gt;
&lt;li&gt;Fake database: in-memory for fast experimentation/prototyping&lt;/li&gt;
&lt;li&gt;SQLite: in-memory and disk&lt;/li&gt;
&lt;li&gt;MySQL: server based&lt;/li&gt;
&lt;li&gt;Block storage: file system, storage area network, or cloud based&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evolving-data&#34;&gt;Evolving Data&lt;/h2&gt;
&lt;h3 id=&#34;recall-schema&#34;&gt;Recall schema&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2075.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2075.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;iterative-schema-development-and-evolution&#34;&gt;Iterative schema development and evolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2076.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2076.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;reliability-during-data-evolution&#34;&gt;Reliability during data evolution&lt;/h3&gt;
&lt;p&gt;Platform needs to be resilient to disruptions from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inconsistent data&lt;/li&gt;
&lt;li&gt;Software&lt;/li&gt;
&lt;li&gt;User configurations&lt;/li&gt;
&lt;li&gt;Execution environments&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scalability-during-data-evolution&#34;&gt;Scalability during data evolution&lt;/h3&gt;
&lt;p&gt;Platform must scale during:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High data volume during training&lt;/li&gt;
&lt;li&gt;Variable request traffic during serving&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anomaly-detection-during-data-evolution&#34;&gt;Anomaly detection during data evolution&lt;/h3&gt;
&lt;p&gt;Platform designed with these principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;easy to detect anomalies&lt;/li&gt;
&lt;li&gt;data errors treated same as code bugs&lt;/li&gt;
&lt;li&gt;Updata data schema&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schema-inspection-during-data-evolution&#34;&gt;Schema inspection during data evolution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Looking at schema versions to track data evolution&lt;/li&gt;
&lt;li&gt;Schema can drive other automated processes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-schema-versions&#34;&gt;Multiple schema versions&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2077.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2077.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;maintaining-varieties-of-schema&#34;&gt;Maintaining varieties of schema&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;business use-case needs to support data from different sources&lt;/li&gt;
&lt;li&gt;Data evolves rapidly&lt;/li&gt;
&lt;li&gt;Is anomaly part of accepted type of data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schema-environments&#34;&gt;Schema environments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Customize the schema for each environment&lt;/li&gt;
&lt;li&gt;Ex: add or remove label in schema based on type of dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;enterprise-data-storage&#34;&gt;Enterprise Data Storage&lt;/h2&gt;
&lt;h3 id=&#34;feature-stores&#34;&gt;Feature stores&lt;/h3&gt;
&lt;p&gt;Many modeling problems use identical or similar features&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2078.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2078.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2079.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2079.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;avoid duplication&lt;/li&gt;
&lt;li&gt;control access&lt;/li&gt;
&lt;li&gt;purge&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;offline-feature-processing&#34;&gt;Offline feature processing&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2080.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2080.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;online-feature-usage&#34;&gt;Online feature usage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Low latency access to features&lt;/li&gt;
&lt;li&gt;Features difficult to compute online&lt;/li&gt;
&lt;li&gt;Precompute and store for low latency access&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;features-for-online-serving-batch&#34;&gt;Features for online serving: batch&lt;/h3&gt;
&lt;p&gt;Batch precomputing and loading history&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple and efficient&lt;/li&gt;
&lt;li&gt;Works well for features to only be updated every few hours or once a day&lt;/li&gt;
&lt;li&gt;Same data is used for training and serving&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-store-key-aspects&#34;&gt;Feature store: key aspects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;managing feature data from a single person to large enterprises&lt;/li&gt;
&lt;li&gt;Scalable and performant access to feature data in training and servign&lt;/li&gt;
&lt;li&gt;Provide consistent and point-in-time correct access to feature data&lt;/li&gt;
&lt;li&gt;Enable discovery, documentation, and insights into your features&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-warehouse&#34;&gt;Data warehouse&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Aggregates data sources&lt;/li&gt;
&lt;li&gt;Processed and analyzed&lt;/li&gt;
&lt;li&gt;read optimized&lt;/li&gt;
&lt;li&gt;Not real time&lt;/li&gt;
&lt;li&gt;Follows schema&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-features-of-data-warehouse&#34;&gt;Key features of data warehouse&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Subject oriented&lt;/li&gt;
&lt;li&gt;Integrated&lt;/li&gt;
&lt;li&gt;Non volatile&lt;/li&gt;
&lt;li&gt;time variant&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantages-of-data-warehouse&#34;&gt;Advantages of data warehouse&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Enhanced ability to analyze data&lt;/li&gt;
&lt;li&gt;Timely access to data&lt;/li&gt;
&lt;li&gt;Enhanced data quality and consistency&lt;/li&gt;
&lt;li&gt;High return on investment&lt;/li&gt;
&lt;li&gt;Increased query and system performance&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-with-databases&#34;&gt;Comparison with databases&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2081.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2081.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-lakes&#34;&gt;Data lakes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Aggregates raw data from one or more sources&lt;/li&gt;
&lt;li&gt;Data can be structured or unstructured&lt;/li&gt;
&lt;li&gt;Doesn&amp;rsquo;t involve any processing before writing data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-with-data-warehouse&#34;&gt;Comparison with data warehouse&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2082.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2082.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-points-9&#34;&gt;Key points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feature store: central repository for storing documented, curated, and access-controlled features, specifically for ML&lt;/li&gt;
&lt;li&gt;Data warehouse: subject-oriented repository of structured data optimized for fast read&lt;/li&gt;
&lt;li&gt;Data Lakes: repository of data stored in its natural and raw format&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reading&#34;&gt;Reading&lt;/h2&gt;
&lt;p&gt;If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won’t have to read these to complete this week’s practice quizzes.&lt;/p&gt;
&lt;p&gt;Data Versioning:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://dvc.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dvc.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-lfs.github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://git-lfs.github.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ML Metadata:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tfx/guide/mlmd#data_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/tfx/guide/mlmd#data_model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tfx/guide/understanding_custom_components&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/tfx/guide/understanding_custom_components&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Chicago taxi trips data set:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/covertype&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://archive.ics.uci.edu/ml/datasets/covertype&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Feast:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/feast-dev/feast&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/feast-dev/feast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;week-4&#34;&gt;Week 4&lt;/h1&gt;
&lt;h2 id=&#34;advanced-labeling&#34;&gt;Advanced Labeling&lt;/h2&gt;
&lt;h3 id=&#34;why-is-advanced-labeling-important&#34;&gt;Why is advanced labeling important&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;manually labeling of data is expensive&lt;/li&gt;
&lt;li&gt;Unlabeled data is usually cheap and easy to get&lt;/li&gt;
&lt;li&gt;Unlabeled data contains a lot of information that can improve our model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;semi-supervised-learning&#34;&gt;Semi-supervised learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2083.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2083.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advantages
&lt;ul&gt;
&lt;li&gt;Combining labeled and unlabeled data boosts accuracy&lt;/li&gt;
&lt;li&gt;Getting unlabeled data is cheap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;label-propagation&#34;&gt;Label propagation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Semi-supervised ML algorithm&lt;/li&gt;
&lt;li&gt;A subset of the examples have labels&lt;/li&gt;
&lt;li&gt;Labels are propagated to the unlabeled points:
&lt;ul&gt;
&lt;li&gt;Based on similarity of &amp;ldquo;community structure&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;label-propagation---graph-based&#34;&gt;Label propagation - graph based&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2084.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2084.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlabeled examples can be assigned labels based on their neighbors&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;active-learning&#34;&gt;Active learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A family of algorithms for intelligently sampling data&lt;/li&gt;
&lt;li&gt;Select the points to be labeled that would be most informative for model training&lt;/li&gt;
&lt;li&gt;Very helpful in the following situations:
&lt;ul&gt;
&lt;li&gt;Constrained data budgets: you can only afford labeling a few points&lt;/li&gt;
&lt;li&gt;Imbalanced dataset: helps selecting rare classes for training&lt;/li&gt;
&lt;li&gt;Target metrics: when baseline sampling strategy does not improve selected metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;active-learning-strategies&#34;&gt;Active learning strategies&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2085.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2085.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;active-learning-cycle&#34;&gt;Active learning cycle&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2086.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2086.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;margin-sampling&#34;&gt;Margin sampling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2087.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2087.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2088.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2088.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;repeat until the performance doesn&amp;rsquo;t improve&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2089.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2089.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;active-learning-sampling-techniques&#34;&gt;Active learning sampling techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Margin sampling: label points the current model  is least confident in&lt;/li&gt;
&lt;li&gt;Cluster-based sampling: sample from well-formed clusters to &amp;lsquo;cover&amp;rsquo; the entire space&lt;/li&gt;
&lt;li&gt;Query-by-committee: train an ensemble of models and sample points that generate disagreement&lt;/li&gt;
&lt;li&gt;Region-based sampling: runs several active learning algorithms in different partitions of the space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weak-supervision&#34;&gt;Weak supervision&lt;/h3&gt;
&lt;p&gt;Weak supervision is about leveraging higher-level and/or noisier input from subject matter experts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlabeled data, without ground-truth labels&lt;/li&gt;
&lt;li&gt;One or more weak supervision sources
&lt;ul&gt;
&lt;li&gt;a list of heuristics that can automate labeling&lt;/li&gt;
&lt;li&gt;Typically provided by subject matter experts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Noisy labels have a certain probability of being correct, not 100%&lt;/li&gt;
&lt;li&gt;Objective: learn a generative model to determine weights for weak supervision sources&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;snorkel&#34;&gt;Snorkel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Programmatically building and managing training datasets without manual labeling&lt;/li&gt;
&lt;li&gt;Automatically: models, cleans, and integrates the resulting training data&lt;/li&gt;
&lt;li&gt;Applies novel, theoretically-grounded techniques&lt;/li&gt;
&lt;li&gt;Also offers data augmentation and slicing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-programming-pipeline-in-snorkel&#34;&gt;Data programming pipeline in Snorkel&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2090.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2090.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h2&gt;
&lt;h3 id=&#34;how-do-you-get-more-data&#34;&gt;How do you get more data?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Augmentation as a way to expand datasets&lt;/li&gt;
&lt;li&gt;One way is introducing minor alterations&lt;/li&gt;
&lt;li&gt;For images: flips, rotations, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-does-augmentation-data-help&#34;&gt;How does augmentation data help?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Adds examples that are similar to real examples&lt;/li&gt;
&lt;li&gt;Improves coverage of feature space&lt;/li&gt;
&lt;li&gt;Beware of invalid augmentations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-advanced-techniques&#34;&gt;Other advanced techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Semi-supervised data augmentation. e.g., UDA, semi-supervised learning with GANs&lt;/li&gt;
&lt;li&gt;Policy-based data augmentation e.g., AutoAugment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2091.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2091.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-different-data-types&#34;&gt;Preprocessing Different Data Types&lt;/h2&gt;
&lt;h3 id=&#34;different-types-of-data&#34;&gt;Different types of data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TRX pre-processing capabilities for multiple data types
&lt;ul&gt;
&lt;li&gt;Images&lt;/li&gt;
&lt;li&gt;Video&lt;/li&gt;
&lt;li&gt;text&lt;/li&gt;
&lt;li&gt;audio&lt;/li&gt;
&lt;li&gt;time series&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-data&#34;&gt;Time series data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2092.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2092.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;time-series-forecasting&#34;&gt;Time series forecasting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;predicts future events by analyzing data from the past&lt;/li&gt;
&lt;li&gt;makes predictions on data indexed by time&lt;/li&gt;
&lt;li&gt;example:
&lt;ul&gt;
&lt;li&gt;predict future temperature at a given location based on historical meteorological data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-dataset-weather-prediction&#34;&gt;Time series dataset: weather prediction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;to preprocess time series data with tensorflow transform&lt;/li&gt;
&lt;li&gt;to convert data int sequences of time steps
&lt;ul&gt;
&lt;li&gt;making data ready to train a long short-term memory recurrent neural network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sensors-and-signals&#34;&gt;Sensors and signals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Signals are sequences of data collected from real time sensors&lt;/li&gt;
&lt;li&gt;Each data point is indexed by a timestamp&lt;/li&gt;
&lt;li&gt;Sensors and signals data is thus time series data&lt;/li&gt;
&lt;li&gt;Example: classify sequences of accelerometer data recorded by the sensors on smartphones to identify the associate activity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;human-activity-recognition&#34;&gt;Human activity recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;HAR tasks require segmentation operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Raw inertial data from wearables fluctuate greatly over time&lt;/li&gt;
&lt;li&gt;Segmented data should be transformed for modeling&lt;/li&gt;
&lt;li&gt;Different methods of transformation:
&lt;ul&gt;
&lt;li&gt;Spectrograms&lt;/li&gt;
&lt;li&gt;normalization end encoding&lt;/li&gt;
&lt;li&gt;Multichannel&lt;/li&gt;
&lt;li&gt;Fourier transform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2093.png&#34; alt=&#34;Notes%20for%20Machine%20Learning%20Data%20Lifecycle%20in%20Produ%208912ae3a6c01444aa7ed224374b681f1/Untitled%2093.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reading-1&#34;&gt;Reading&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/jeffdean/status/1106325994913189888?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hand Labeling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dawn.cs.stanford.edu/2017/07/16/weak-supervision/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weak supervision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.snorkel.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Snorkel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How do you get more data?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google-research/uda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Techniques&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/lite/models/image_classification/overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Images in tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CIFAR-10&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/cifar10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/datasets/catalog/cifar10&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bgc-jena.mpg.de/wetter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weather dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cis.fordham.edu/wisdm/dataset.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human Activity Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Label Propagation:&lt;/p&gt;
&lt;p&gt;Iscen, A., Tolias, G., Avrithis, Y., &amp;amp; Chum, O. (2019). Label propagation for deep semi-supervised learning. &lt;a href=&#34;https://arxiv.org/pdf/1904.04717.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.04717.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slide 13 active learning:&lt;/p&gt;
&lt;p&gt;Source: Original slides by Yale Cong&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes for Introduction to Machine Learning in Production (MLOps1) on Coursera/Deeplearning.ai</title>
      <link>/post/mlops1/</link>
      <pubDate>Mon, 07 Jun 2021 19:31:34 -0700</pubDate>
      <guid>/post/mlops1/</guid>
      <description>&lt;h1 id=&#34;week-1-steps-of-an-ml-project&#34;&gt;Week 1: Steps of an ML project&lt;/h1&gt;
&lt;h2 id=&#34;the-ml-project-lifecycle&#34;&gt;The ML project lifecycle&lt;/h2&gt;
&lt;p&gt;MLOps (Machine learning Operations) comprises a set of tools and principles to support progress through the ML project lifcycle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;scoping&#34;&gt;Scoping&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Decide to work on speech recognition for voice search&lt;/li&gt;
&lt;li&gt;Decide on key metrics:
&lt;ul&gt;
&lt;li&gt;Acc, latency, throughput&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Estimate resources and timeline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;define-data&#34;&gt;Define data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Is the data labeled consistently&lt;/li&gt;
&lt;li&gt;How much silence before/after each clip?&lt;/li&gt;
&lt;li&gt;How to perform volume normalization?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;modeling&#34;&gt;Modeling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%201.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%201.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%202.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%202.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-drift-and-concept-drift&#34;&gt;Data drift and concept drift&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Data drift&lt;/strong&gt;: the input data has changed. The distribution of the variables is meaningfully different. As a result, the trained model is not relevant for this new data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Concept drift&lt;/strong&gt; occurs when the patterns the model learned no longer hold.&lt;/p&gt;
&lt;p&gt;In contrast to the data drift, the distributions (such as user demographics, frequency of words, etc.) might even remain the same. Instead, the &lt;a href=&#34;https://evidentlyai.com/blog/evidently-014-target-and-prediction-drift&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;relationships between the model inputs and outputs change&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;software-engineering-issues&#34;&gt;Software engineering issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Realtime of Batch&lt;/li&gt;
&lt;li&gt;Cloud vs. Edge/Browser&lt;/li&gt;
&lt;li&gt;Compute resources (CPU/GPU/memory)&lt;/li&gt;
&lt;li&gt;Latency, throughput (QPS)&lt;/li&gt;
&lt;li&gt;Logging&lt;/li&gt;
&lt;li&gt;Security and privacy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-deployment-cases&#34;&gt;Common deployment cases&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;New product/capability&lt;/li&gt;
&lt;li&gt;Automate/assist with manual task&lt;/li&gt;
&lt;li&gt;Replace previous ML system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key idesa:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradual ramp up with monitoring&lt;/li&gt;
&lt;li&gt;Rollback&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shadow-mode&#34;&gt;Shadow mode&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ML system shadows the human and runs in parallel.&lt;/li&gt;
&lt;li&gt;Ml system&amp;rsquo;s output not used for any decisions during this phase.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;canary-deployment&#34;&gt;Canary deployment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Roll out to small fraction (say 5%) of traffic initially.&lt;/li&gt;
&lt;li&gt;Monitor system and ramp up traffic gradually.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blue-green-deployment&#34;&gt;Blue green deployment&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%203.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%203.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy way to enable rollback&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;degrees-of-automation&#34;&gt;Degrees of automation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%204.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%204.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;monitoring-dashboard&#34;&gt;Monitoring dashboard&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%205.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%205.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brainstorm the things that could go wrong.&lt;/li&gt;
&lt;li&gt;Brainstorm a few statistics/metrics that will detect the problem.&lt;/li&gt;
&lt;li&gt;It is ok to use many metrics initially and gradually remove the ones you find not useful.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples-of-metrics-to-track&#34;&gt;Examples of metrics to track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Software metrics: memory, compute, latency, throughout, server load&lt;/li&gt;
&lt;li&gt;Input metrics: avg input length, avg input volume, num missing values, avg image brightness&lt;/li&gt;
&lt;li&gt;Output metrics # times return &amp;lsquo;&#39;(null), # times user&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;just-as-ml-modeling-is-iterative-so-is-deployment&#34;&gt;Just as ML modeling is iterative, so is deployment&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%206.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%206.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;monitoring-dashboard-1&#34;&gt;Monitoring dashboard&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Set thresholds for alarms&lt;/li&gt;
&lt;li&gt;Adapt metrics and thresholds over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-maintenance&#34;&gt;Model maintenance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%207.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%207.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manual retraining&lt;/li&gt;
&lt;li&gt;Automatic retraining&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;metrics-to-monitor&#34;&gt;Metrics to monitor&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Monitor
&lt;ul&gt;
&lt;li&gt;Software metrics&lt;/li&gt;
&lt;li&gt;Input metrics&lt;/li&gt;
&lt;li&gt;Output metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How quickly do they change?
&lt;ul&gt;
&lt;li&gt;User data generally has slower drift.&lt;/li&gt;
&lt;li&gt;enterprise data (B2B applications) can shift fast.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code-link-github-repohttpsgithubcomhttps-deeplearning-aimlep-publictreemaincourse1week1-ungraded-lab&#34;&gt;Code Link : &lt;a href=&#34;https://github.com/https-deeplearning-ai/MLEP-public/tree/main/course1/week1-ungraded-lab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github Repo&lt;/a&gt;&lt;/h3&gt;
&lt;h3 id=&#34;reading-material-week-1&#34;&gt;Reading Material Week 1:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning in Production: Why You Should Care About Data and Concept Drift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Monitoring Machine Learning Models in Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=06-AZXmwHjo&amp;amp;ab_channel=DeepLearningAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Chat with Andrew on MLOps: From Model-centric to Data-centric AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;week-2-select-and-train-model&#34;&gt;Week 2: Select and train model&lt;/h1&gt;
&lt;h2 id=&#34;selecting-and-training-a-model&#34;&gt;Selecting and Training a Model&lt;/h2&gt;
&lt;h3 id=&#34;model-development-is-an-iterative-process&#34;&gt;Model development is an iterative process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%208.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%208.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;challenges-in-model-development&#34;&gt;Challenges in model development&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Doing well on training set (usually measured by average training error)&lt;/li&gt;
&lt;li&gt;Doing well on dev/test sets.&lt;/li&gt;
&lt;li&gt;Doing well on business metrics/project goals.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;performance-on-disproportionately-important-example&#34;&gt;Performance on disproportionately important example&lt;/h3&gt;
&lt;p&gt;Web search example&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Informational and transactional queries&lt;/li&gt;
&lt;li&gt;Navigational queries&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-on-key-slices-of-the-dataset&#34;&gt;Performance on key slices of the dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Example: ML for loan approval: makes sure not to discriminate by ethnicity, gender, location, language of other protected attributes.&lt;/li&gt;
&lt;li&gt;Example: Product recommendations from retailers: Be careful to treat fairly all major user, retailer, and product categories.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rare-classes&#34;&gt;Rare classes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Skewed data distribution&lt;/li&gt;
&lt;li&gt;Accuracy in rare classes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unfortunate-conversation-in-many-companies&#34;&gt;Unfortunate conversation in many companies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I did well on the test set&lt;/li&gt;
&lt;li&gt;But this doesn&amp;rsquo;t work for my application&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;establishing-a-baseline-level-of-performance&#34;&gt;Establishing a baseline level of performance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%209.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%209.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;unstructured-and-structured-data&#34;&gt;Unstructured and structured data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unstructured data: Image, Audio, Text (HLP is important)&lt;/li&gt;
&lt;li&gt;Structured data: a data frame&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ways-to-establish-a-baseline&#34;&gt;Ways to establish a baseline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Human level performance&lt;/li&gt;
&lt;li&gt;Literature search for state-of-the-art/open source&lt;/li&gt;
&lt;li&gt;Quick-and-dirty implementation&lt;/li&gt;
&lt;li&gt;Performance of older system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Baseline helps to indicates what might be possible. In some cases (such as HLP) is also gives a sense of what is irreducible error/Bayes error.&lt;/p&gt;
&lt;h3 id=&#34;getting-started-on-modeling&#34;&gt;Getting started on modeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Literature search to see what&amp;rsquo;s possible (courses, blogs, open-source projects).&lt;/li&gt;
&lt;li&gt;Find open-source implementations if available.&lt;/li&gt;
&lt;li&gt;A reasonable algorithm with good data will often outperform a great algorithm with no so good data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deployment-constraints-when-picking-a-model&#34;&gt;Deployment constraints when picking a model&lt;/h3&gt;
&lt;p&gt;Should you take into account deployment constraints when picking a model?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yes, if baseline is already established and goal is to build and deploy.&lt;/li&gt;
&lt;li&gt;No (or not necessarily), if purpose is to establish a baseline and determine what is possible and might be worth pursuing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sanity-check-for-code-and-algorithm&#34;&gt;Sanity-check for code and algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try to overfit a small training dataset before training on a large one.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;error-analysis-and-performance-auditing&#34;&gt;Error analysis and performance auditing&lt;/h2&gt;
&lt;h3 id=&#34;error-analysis-example&#34;&gt;Error analysis example&lt;/h3&gt;
&lt;h3 id=&#34;speech-recognition-example&#34;&gt;Speech recognition example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2010.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2010.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;iterative-process-of-error-analysis&#34;&gt;Iterative process of error analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2011.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2011.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;useful-metrics-for-each-tag&#34;&gt;Useful metrics for each tag&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What fraction of errors has that tag?&lt;/li&gt;
&lt;li&gt;Of all data with that tag, what fraction is misclassified?&lt;/li&gt;
&lt;li&gt;What fraction of all the data has that tag?&lt;/li&gt;
&lt;li&gt;How much room for improvement is there on data with that tag?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prioritizing-what-to-work-on&#34;&gt;Prioritizing what to work on&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2012.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2012.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Decide on most important categories to work on based on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How much room for improvement there is&lt;/li&gt;
&lt;li&gt;How frequently that category appears&lt;/li&gt;
&lt;li&gt;How easy is to improve accuracy in that category&lt;/li&gt;
&lt;li&gt;How important it is to improve in that category&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;addingimproving-data-for-specific-categories&#34;&gt;Adding/improving data for specific categories&lt;/h3&gt;
&lt;p&gt;For categories you want to prioritize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect more data&lt;/li&gt;
&lt;li&gt;Use data augmentation to get more data&lt;/li&gt;
&lt;li&gt;Improve label accuracy/data quality&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;skewed-datasets&#34;&gt;Skewed datasets&lt;/h2&gt;
&lt;h3 id=&#34;confusion-matrix-precision-and-recall&#34;&gt;Confusion matrix: Precision and Recall&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2013.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2013.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-happens-with-print0&#34;&gt;What happens with print(&amp;lsquo;0&amp;rsquo;)?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2014.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2014.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;combining-precision-and-recall--f1-score&#34;&gt;Combining precision and recall — F1 score&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2015.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2015.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-class-metrics&#34;&gt;Multi-class metrics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2016.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2016.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;performance-auditing&#34;&gt;Performance auditing&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2017.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2017.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;auditing-framework&#34;&gt;Auditing framework&lt;/h3&gt;
&lt;p&gt;Check for accuracy, fairness/bias, and other problems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Brainstorm the ways the system might go wrong.
&lt;ul&gt;
&lt;li&gt;Performance on subsets of data (e.g., ethnicity, gender).&lt;/li&gt;
&lt;li&gt;How common are certain errors (e.g., FP, FN).&lt;/li&gt;
&lt;li&gt;Performance on rare classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Establish metrics to assess performance against these issues on appropriate slices of data.&lt;/li&gt;
&lt;li&gt;Get business/product owner buy-in.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;speech-recognition-example-1&#34;&gt;Speech recognition example&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Brainstorm the ways the system might go wrong.
&lt;ul&gt;
&lt;li&gt;Accuracy on different genders and ethnicities.&lt;/li&gt;
&lt;li&gt;Accuracy on different devices.&lt;/li&gt;
&lt;li&gt;Prevalence of rude mis-transcriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Establish metrics to assess performance against these issues on appropriate slices of data.
&lt;ul&gt;
&lt;li&gt;Mean accuracy for different genders and major accents.&lt;/li&gt;
&lt;li&gt;Mean accuracy on different devices.&lt;/li&gt;
&lt;li&gt;Check for prevalence of offensive words in the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-iteration&#34;&gt;Data iteration&lt;/h2&gt;
&lt;h3 id=&#34;data-centric-ai-development&#34;&gt;Data-centric AI development&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model-centric view: take the data you have, and develop a model that does as well as possible on it.
&lt;ul&gt;
&lt;li&gt;Hold the data fixed and iteratively improve the code/model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data-centric view: the quality of the data is paramount. Use tools to improve the data quality; this will allow multiple models to do well.
&lt;ul&gt;
&lt;li&gt;Hold the code fixed and iteratively improve the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-useful-picture-of-data-augmentation&#34;&gt;A useful picture of data augmentation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2018.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2018.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-augmentation&#34;&gt;Data augmentation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Goal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create realistic examples that (i) the algorithm does poorly on, but (ii) humans (or other baseline) do well on&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Checklist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does it sound realistic?&lt;/li&gt;
&lt;li&gt;Is the x→ y mapping clear? (e.g. can humans recognize speech?)&lt;/li&gt;
&lt;li&gt;Is the algorithm currently doing poorly on it?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2019.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2019.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-iteration-loop&#34;&gt;Data iteration loop&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2020.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2020.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;can-adding-data-hurt-performance&#34;&gt;Can adding data hurt performance?&lt;/h3&gt;
&lt;p&gt;For unstructured data problems, if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The model is large (low bias).&lt;/li&gt;
&lt;li&gt;The mapping x→y is clear (e.g., given only the input x, humans can make accurate predictions).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, adding data rarely hurts accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2021.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2021.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;adding-features-to-structured-data&#34;&gt;Adding features to structured data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Restaurant recommendation example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vegan are frequently recommended restaurants with only meat options.&lt;/li&gt;
&lt;li&gt;Possible features to add?
&lt;ul&gt;
&lt;li&gt;Is person vegan (based on past orders)?&lt;/li&gt;
&lt;li&gt;Does restaurant have vegan options (based on menu)?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other food delivery examples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only tea/coffee and only pizza&lt;/li&gt;
&lt;li&gt;What are the added features that can help make a decision?&lt;/li&gt;
&lt;li&gt;Product recommendation:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Collaborative filtering ——&amp;gt; Content based filtering (cold-start)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-iteration-1&#34;&gt;Data iteration&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2022.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2022.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Error analysis can be harder if there is not good baseline (such as HLP) to compare to.&lt;/li&gt;
&lt;li&gt;Error analysis, user feedback and benchmarking to competitors can all provide inspiration for features to add.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experiment-tracking&#34;&gt;Experiment tracking&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What to track?
&lt;ul&gt;
&lt;li&gt;Algorithm/code versioning&lt;/li&gt;
&lt;li&gt;Dataset used&lt;/li&gt;
&lt;li&gt;Hyperparameters&lt;/li&gt;
&lt;li&gt;Results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tracking tools
&lt;ul&gt;
&lt;li&gt;Text files&lt;/li&gt;
&lt;li&gt;spreadsheet&lt;/li&gt;
&lt;li&gt;Experiment tracking system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Desirable features
&lt;ul&gt;
&lt;li&gt;Information needed to replicate results&lt;/li&gt;
&lt;li&gt;Experiment results, ideally with summary metrics/analysis&lt;/li&gt;
&lt;li&gt;Perhaps also: resource monitoring, visualization, model error analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;from-big-data-to-good-data&#34;&gt;From big data to good data&lt;/h3&gt;
&lt;p&gt;Try to ensure consistently high-quality data in all phases of the ML project lifecycly&lt;/p&gt;
&lt;p&gt;Good data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Covers important cases (good coverage of inputs x)&lt;/li&gt;
&lt;li&gt;Is defined consistently (definition of labels y is unambiguous)&lt;/li&gt;
&lt;li&gt;Has timely feedback from production data (distribution covers data drift and concept drift)&lt;/li&gt;
&lt;li&gt;Is sized appropriately&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code-link--google-colabhttpscolabresearchgooglecomgithubhttps-deeplearning-aimlep-publicblobmaincourse1week2-ungraded-labc1w2_ungraded_lab_birds_cats_dogsipynb&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/https-deeplearning-ai/MLEP-public/blob/main/course1/week2-ungraded-lab/C1W2_Ungraded_Lab_Birds_Cats_Dogs.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code Link : Google Colab&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Reading Material Week 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.ml.cmu.edu/2020/08/31/3-baselines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Establishing a baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://techcommunity.microsoft.com/t5/azure-ai/responsible-machine-learning-with-error-analysis/ba-p/2141774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Error analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://neptune.ai/blog/ml-experiment-tracking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Experiment tracking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;week-3-data-definition-and-baseline&#34;&gt;Week 3: Data Definition and Baseline&lt;/h1&gt;
&lt;h2 id=&#34;define-data-and-establish-baseline&#34;&gt;Define Data and Establish Baseline&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2023.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2023.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-definition-questions&#34;&gt;Data definition questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the input x?
&lt;ul&gt;
&lt;li&gt;lightning? contrast? resolution?&lt;/li&gt;
&lt;li&gt;What features need to be in included?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is the target label y?
&lt;ul&gt;
&lt;li&gt;How can we ensure labelers give consistent labels?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;major-types-of-data-problems&#34;&gt;Major types of data problems&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2024.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2024.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;unstructured-vs-structured-data&#34;&gt;Unstructured vs. structured data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unstructured data
&lt;ul&gt;
&lt;li&gt;may or may not have huge collection of unlabeled examples x.&lt;/li&gt;
&lt;li&gt;Humans can label more data.&lt;/li&gt;
&lt;li&gt;Data augmentation more likely to be helpful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Structured data
&lt;ul&gt;
&lt;li&gt;May be more difficult to obtain more data.&lt;/li&gt;
&lt;li&gt;Human labelling may not be possible (with some exceptions)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;small-data-vs-big-data&#34;&gt;Small data vs. big data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Small data
&lt;ul&gt;
&lt;li&gt;Clean labels are critical&lt;/li&gt;
&lt;li&gt;Can manually look through dataset and fix labels&lt;/li&gt;
&lt;li&gt;Can get all the labelers to talk to each other&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Big data
&lt;ul&gt;
&lt;li&gt;Emphasis data process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-label-consistency-is-important&#34;&gt;Why label consistency is important?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2025.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2025.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2026.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2026.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;big-data-problems-can-have-small-data-challenges-too&#34;&gt;Big data problems can have small data challenges too&lt;/h3&gt;
&lt;p&gt;Problems with a large dataset but where there&amp;rsquo;s a long tail or rare events in the input will have small data challenges too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web search&lt;/li&gt;
&lt;li&gt;Self-driving cars&lt;/li&gt;
&lt;li&gt;Product recommendation systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;improving-label-consistency&#34;&gt;Improving label consistency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have multiple labelers label same example.&lt;/li&gt;
&lt;li&gt;When there is disagreement, have MLE, subject matter expert (SME) and/or labelers discuss definition of y to reach agreement.&lt;/li&gt;
&lt;li&gt;If labelers believe that x doesn&amp;rsquo;t contain enough information, consider changing x.&lt;/li&gt;
&lt;li&gt;Iterate until it is hard to significantly increase agreement&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2027.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2027.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;have-a-classlabel-to-capture-uncertainty&#34;&gt;Have a class/label to capture uncertainty&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2028.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2028.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;small-data-vs-big-data-unstructured-data&#34;&gt;Small data vs. big data (unstructured data)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Small data
&lt;ul&gt;
&lt;li&gt;Usually small number of labelers&lt;/li&gt;
&lt;li&gt;Can ask labelers to discuss specific labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Big data
&lt;ul&gt;
&lt;li&gt;Get to consistent definition with a small group.&lt;/li&gt;
&lt;li&gt;Then send labeling instructions to labelers.&lt;/li&gt;
&lt;li&gt;Can consider having multiple labelers label every example and using voting or consensus labels to increase accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-measure-hlp&#34;&gt;Why measure HLP?&lt;/h3&gt;
&lt;p&gt;Estimate Bayes error / irreducible error to help with error analysis and prioritization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2029.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2029.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-uses-of-hlp&#34;&gt;Other uses of HLP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In academia, establish and beat a respectable benchmark to support publication.&lt;/li&gt;
&lt;li&gt;Business or product owner asks for 99% accuracy. HLP helps establish a more reasonable target.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Prove&amp;rdquo; the ML system is superior to humans doing the job and thus the business or product owner should adopt it. (Use with caution)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-problem-with-beating-hlp-as-a-proof-or-ml-superiority&#34;&gt;The problem with beating HLP as a &amp;lsquo;proof&amp;rsquo; or ML &amp;lsquo;superiority&amp;rsquo;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2030.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2030.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;raising-hlp&#34;&gt;Raising HLP&lt;/h3&gt;
&lt;p&gt;When the ground truth label is externally defined, HLP gives an estimate for Bayes error / irreducible error.&lt;/p&gt;
&lt;p&gt;But often ground truth is just anther human label.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the label y comes from a human label, HLP &amp;laquo; 100% may indicate ambiguous labeling instructions.&lt;/li&gt;
&lt;li&gt;Improving label consistency will raise HLP&lt;/li&gt;
&lt;li&gt;This makes it harder for ML to beat HLP. But the more consistent labels will raise ML performance, which is ultimately likely to benefit the actual application performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hlp-on-structured-data&#34;&gt;HLP on structured data&lt;/h3&gt;
&lt;p&gt;Structured data problems are less likely to involve human labelers, thus HLP is less frequently used.&lt;/p&gt;
&lt;p&gt;Some exceptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User ID merging: same person?&lt;/li&gt;
&lt;li&gt;Based on network traffic, is the computer hacked?&lt;/li&gt;
&lt;li&gt;Is the transaction fraudulent?&lt;/li&gt;
&lt;li&gt;Spam account? Bot?&lt;/li&gt;
&lt;li&gt;From GPS, what is the mode transportation - on foot, bike, car, bus?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;label-and-organize-data&#34;&gt;Label and Organize Data&lt;/h2&gt;
&lt;h3 id=&#34;how-long-should-you-spend-obtaining-data&#34;&gt;How long should you spend obtaining data?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2031.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2031.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get into this iteration loop as quickly as possible.&lt;/li&gt;
&lt;li&gt;Instead of asking: how long it would take to obtain m examples? ask: How much data can we obtain in k days.&lt;/li&gt;
&lt;li&gt;Exception: if you have worked on the problem before and from experience you know you need m examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventory-data&#34;&gt;Inventory data&lt;/h3&gt;
&lt;p&gt;Brainstorm list of data sources&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2032.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2032.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Other factors: data quality, privacy, regulatory constrains&lt;/p&gt;
&lt;h3 id=&#34;labeling-data&#34;&gt;Labeling data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Options: in-house vs. outsourced vs. crowdsourced&lt;/li&gt;
&lt;li&gt;Having MLEs label data expensive. But doing this for just a few days is usually fine&lt;/li&gt;
&lt;li&gt;Who is qualified to label?
&lt;ul&gt;
&lt;li&gt;Speech recognition - any reasonable fluent speaker&lt;/li&gt;
&lt;li&gt;Factory inspection, medical image diagnosis - SME (subject matter expert)&lt;/li&gt;
&lt;li&gt;Recommender systems - maybe impossible to label well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t increase data by more than 10x at a time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-pipeline-example&#34;&gt;Data pipeline example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2033.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2033.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2034.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2034.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;poc-and-production-phases&#34;&gt;POC and Production phases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;POC(proof-of-concept):
&lt;ul&gt;
&lt;li&gt;Goal is to decide if the application is workable and worth deploying.&lt;/li&gt;
&lt;li&gt;Focus on getting the prototype to work&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s ok if data pre-processing is manual. But take extensive notes/comments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Production phase:&lt;/li&gt;
&lt;li&gt;After project utility is established, use more sophisticated tools to make sure the data pipeline is replicable.&lt;/li&gt;
&lt;li&gt;E.g., Tensor Flow Transform, Apache Beam, Airflow, &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-pipeline-example-1&#34;&gt;Data pipeline example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2035.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2035.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;meta-data&#34;&gt;Meta-data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Examples:
&lt;ul&gt;
&lt;li&gt;Manufacturing visual inspection: time, factory, line #, camera settings, phone model, inspector ID,&amp;hellip;&lt;/li&gt;
&lt;li&gt;Speech recognition: device type, labeler ID, VAD model ID,&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Useful for:
&lt;ul&gt;
&lt;li&gt;Error analysis. Spotting unexpected effects.&lt;/li&gt;
&lt;li&gt;Keeping track of data provenance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;balanced-traindevtest-splits-in-small-data-problems&#34;&gt;Balanced train/dev/test/ splits in small data problems&lt;/h3&gt;
&lt;p&gt;Visual inspection example: 100 examples, 30 positive (defective)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train/dev/test: 60%/20%/20%&lt;/li&gt;
&lt;li&gt;Random split: positive example: 21/2/7 (35%/10%/35%)→ dev set is not representative&lt;/li&gt;
&lt;li&gt;Want: 18/6/6 (30%/30%/30%) →balanced split&lt;/li&gt;
&lt;li&gt;No need to worry about this with large datasets - a random split will be representative&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;scooping&#34;&gt;Scooping&lt;/h2&gt;
&lt;h3 id=&#34;what-is-scoping&#34;&gt;What is scoping?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2036.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2036.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;scoping-process&#34;&gt;Scoping process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2037.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2037.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;separating-problem-identification-from&#34;&gt;Separating problem identification from&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2038.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2038.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;feasibility-is-this-project-technically-feasible&#34;&gt;Feasibility: Is this project technically feasible?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use external benchmark (literature, other company, competitor)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2039.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2039.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-use-hlp-to-benchmark&#34;&gt;Why use HLP to benchmark?&lt;/h3&gt;
&lt;p&gt;People are very good on unstructured data tasks&lt;/p&gt;
&lt;p&gt;Criteria: can a human, given the same data, perform the task?&lt;/p&gt;
&lt;h3 id=&#34;do-we-have-features-that-are-predictive&#34;&gt;Do we have features that are predictive?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Given past purchases, predict future purchases ✅&lt;/li&gt;
&lt;li&gt;Given weather, predict shopping mall foot traffic ✅&lt;/li&gt;
&lt;li&gt;Given DNA info, predict heart disease ❓&lt;/li&gt;
&lt;li&gt;Given social media chatter, predict demand for a clothing style ❓&lt;/li&gt;
&lt;li&gt;Given history of stock&amp;rsquo;s price, predict future price of that stock ❌&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;history-of-project&#34;&gt;History of project&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2040.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2040.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;diligence-on-value&#34;&gt;Diligence on value&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2041.png&#34; alt=&#34;Notes%20for%20Introduction%20to%20Machine%20Learning%20in%20Prod%2055eeb7a65b57401ba9ba2321757b925b/Untitled%2041.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ethical-considerations&#34;&gt;Ethical considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Is this project creating net positive societal value?&lt;/li&gt;
&lt;li&gt;Is this project reasonable fair and free from bias?&lt;/li&gt;
&lt;li&gt;Have any ethical concerns been openly aired and debated?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;milestones--resourcing&#34;&gt;Milestones &amp;amp; Resourcing&lt;/h3&gt;
&lt;p&gt;Key specifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ML metrics (accuracy, precision/recall, etc.)&lt;/li&gt;
&lt;li&gt;Software metrics (latency, throughput, etc. given compute resources)&lt;/li&gt;
&lt;li&gt;Business metrics (revenue, etc.)&lt;/li&gt;
&lt;li&gt;Resources needed (data, personnel, help from other teams)&lt;/li&gt;
&lt;li&gt;Timeline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If unsure, consider benchmarking to other projects, or building a POC (Proof of Concept) first.&lt;/p&gt;
&lt;h3 id=&#34;colab-httpswwwcourseraorglearnintroduction-to-machine-learning-in-productionungradedlabhndmkdata-labelinglabhttpswwwcourseraorglearnintroduction-to-machine-learning-in-productionungradedlabhndmkdata-labelinglab&#34;&gt;Colab: &lt;a href=&#34;https://www.coursera.org/learn/introduction-to-machine-learning-in-production/ungradedLab/hnDmK/data-labeling/lab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.coursera.org/learn/introduction-to-machine-learning-in-production/ungradedLab/hnDmK/data-labeling/lab&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Reading Material Week 3:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://csgaobb.github.io/Projects/DLDL.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Label ambiguity&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06969.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1706.06969.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cs230.stanford.edu/blog/datapipeline/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data pipelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.tensorflow.org/2021/01/ml-metadata-version-control-for-ml.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data lineage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/ai-machine-learning/key-requirements-for-an-mlops-foundation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLops&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Overall resources:&lt;/p&gt;
&lt;p&gt;Konstantinos, Katsiapis, Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., … Li, Z. (2020). Towards ML Engineering: A brief history of TensorFlow Extended (TFX). &lt;a href=&#34;http://arxiv.org/abs/2010.02013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/2010.02013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paleyes, A., Urma, R.-G., &amp;amp; Lawrence, N. D. (2020). Challenges in deploying machine learning: A survey of case studies. &lt;a href=&#34;http://arxiv.org/abs/2011.09926&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/2011.09926&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decoding memory content from human parietal cortex: VGG16 application on memory research</title>
      <link>/project/nsd/</link>
      <pubDate>Tue, 18 May 2021 16:00:28 -0800</pubDate>
      <guid>/project/nsd/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;When a stimulus from the past is re-encountered, this can elicit increased neural activation relative to a novel stimulus (repetition enhancement) or decreased activation (repetition attenuation). Studies of episodic memory have consistently found that repetition enhancement occurs within parietal cortex and that these enhancement effects are related to behavioral expressions of successful episodic remembering. Repetition attenuation, in contrast, is more typically observed in regions of occipitotemporal cortex and is less consistently related to behavioral expressions of episodic memory. Separately, pattern-based fMRI studies have found that information about the content of stimuli is reflected in both parietal cortex and ventral temporal cortex, but it is less clear how or whether these content representations are integrated with repetition-related memory signals.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;In this study, we utilize the &lt;a href=&#34;http://naturalscenesdataset.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Scene Dataset (NSD)&lt;/a&gt;, which is 7T fMRI study with data size around &lt;strong&gt;2+ TB&lt;/strong&gt;. In this dataset, eight participants performed a continuous recognition task spanning 30-40 fMRI scan sessions and up to 10,000 unique, naturalistic images.
&lt;img src=&#34;fig1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;vectorize-memory-content&#34;&gt;Vectorize memory content&lt;/h3&gt;
&lt;p&gt;In the study, the stimuli subjects remembered were images from &lt;a href=&#34;https://cocodataset.org/#home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COCO dataset&lt;/a&gt;. Thus, to vectorized the memory content information, we can extract information from each image.&lt;/p&gt;
&lt;h4 id=&#34;semantic-models&#34;&gt;Semantic models&lt;/h4&gt;
&lt;p&gt;Since each image from COCO dataset are annotated by human workers with English, we started by analyzing the image content with two popular semantic models: &lt;a href=&#34;https://arxiv.org/abs/1301.3781&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word2vec&lt;/a&gt; and &lt;a href=&#34;https://github.com/oborchers/Fast_Sentence_Embeddings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast sentence embedding (fse)&lt;/a&gt;. However, after comparing the similarity analysis based on the embedding from these two semantic models and our human raters on a small subset of the images, we were not satisfying with these two methods in terms of capturing the image content information.&lt;/p&gt;
&lt;h4 id=&#34;vgg16&#34;&gt;VGG16&lt;/h4&gt;
&lt;p&gt;We believe the undesirable results from the semantic models are due to the quality of the annotation: each image was annotated by five human workers, and each worked wrote one single sentence to describe the image. With this setting, the richness of the annotation is very limited so that the annotation is not representative to the image content. To solve this problem, we believe quantifying the image content directly from the visual part of the image would be a closer approximation of the memory content. After literature review, we decided that &lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VGG16&lt;/a&gt;, a convolutional neural network designed for image content classification and detection, would be a good candidate for our purpose. We passed the images through VGG16 and used the output from the last fully connected layer (fc3) as the content vector of each image. The VGG16 model was trained on Imagenet, which is a different image dataset from COCO. To make sure the difference of image dataset won&amp;rsquo;t be a problem for adopting the neural network in our analysis, I visualized all the images in our experiment based on their VGG16 fc3 feature similarity with t-SNE (see figure below). As can been, the feature output from the last fully connected layer can successfully represent both local and global distance of image similarity. Although the model learned from low level visual features, it can still capture high level information of the image. Here is a &lt;a href=&#34;/post/vgg-viusalization/&#34;&gt;post&lt;/a&gt; I wrote about how to extract features of VGG16 with pytorch and visualize images base on their similarity with t-SNE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;model-prediction&#34;&gt;Model prediction&lt;/h3&gt;
&lt;p&gt;We calculated the difference in multivoxel fMRI activations between the first presentation (initial encoding) and second presentation (retrieval) for each stimulus and tested whether these repetition-related differences in activity patterns carried information about the visual content of the image. As demonstrate previously, to quantify the content of each image, we passed the images through VGG16, a convolutional neural network designed for image content classification and detection. We then reduced features from the output layer to 10 PCA dimensions and used ridge regression to test whether repetition-related changes in fMRI activity patterns predicted the PCA content scores. We used MSE to evaluate model performance, comparing it with 1000 permutation model results.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We found that repetition-related differences in medial and lateral parietal cortex predicted image content
significantly above chance level (1000 permutation test). Moreover, these predictions were significantly better for hits (correct recognition) compared to misses (failed recognition) indicating that the presence of content information was directly related to successful recognition. Interestingly, repetition-related differences in occipitotemporal cortex also predicted image content, but the success of these predictions were less dependent on successful behavioral recognition. Collectively, our results indicate that repetition-related enhancements which have consistently been observed in parietal cortex directly integrate information about the content of what is being remembered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>{roistats}: an R package for fast and easy multiple testing analysis</title>
      <link>/project/roistats/</link>
      <pubDate>Sat, 06 Mar 2021 13:53:48 -0700</pubDate>
      <guid>/project/roistats/</guid>
      <description>&lt;p&gt;While working on my first grad school project, I found in our research field, the analysis of multiple testing is involved in almost every project, since we need to compute same analysis over multiple brain regions. Thus, we need to apply same basic descriptive statistics, different variants of t-tests, and multiple comparison correction to multiple groups.&lt;/p&gt;
&lt;p&gt;Quickly I got tedious about writing similar long pipelines of doing the multiple testing analysis, so I decided to wrap up my pipeline into functions, and combine functions into a package, and {roistats} came out! All the functions from the package can be used in combination with dplyr.&lt;/p&gt;
&lt;p&gt;For the detail of what functions are included and how to use them, check out the website for this package: &lt;a href=&#34;https://irisfee.github.io/roistats/index.html&#34;&gt;https://irisfee.github.io/roistats/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient way of the brain for resolving similar memory interference</title>
      <link>/project/color-differentiation/</link>
      <pubDate>Sun, 21 Feb 2021 15:10:03 -0800</pubDate>
      <guid>/project/color-differentiation/</guid>
      <description>&lt;p&gt;The paper is published on Journal of Neuroscience: &lt;a href=&#34;https://www.jneurosci.org/content/41/13/3014.full#sec-27&#34;&gt;https://www.jneurosci.org/content/41/13/3014.full#sec-27&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Given the vast number of memories that humans store, overlap between memories is inevitable. The similarity between memories could cause confusion when trying to retrieve a certain piece of memory. For example, one student in your stats class is called Mario, while another student in your algorithm class is called Wario. These two guys not only have look similar but also tend to dress in a similar style. You may accidentally call Mario, Wario. However, after several weeks of classes, you can definitely tell Mario and Wario apart because your brain develop some strategies to mange the competition in memory.&lt;/p&gt;
&lt;p&gt;Evidence from recent neuroimaging studies hints at the idea that memory representations are distorted as an adaptive response to interference. Namely, several studies have found that, when similar events are encoded into memory, this triggers a targeted exaggeration of differences in patterns of activity. Yet, a critical limitation of these studies is that the feature dimensions along which memories move are underspecified. That is, do changes in neural representations correspond to changes in the information content of memories?&lt;/p&gt;
&lt;p&gt;Here, we tested whether interference between highly similar memories triggers adaptive distortions in memory representations and corresponding behavioral expressions of memories. Our motivating theoretical perspective was that subtle differences between similar memories are prioritized and exaggerated to reduce the potential for interference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;controlled-experiment-design&#34;&gt;Controlled experiment design&lt;/h2&gt;
&lt;p&gt;In order to answer the question, we used color as the memory feature to probe since that color is 1. continuous and 2. can be reported by participants.&lt;/p&gt;
&lt;p&gt;Specially, we used a 2-day procedure in which participants received extensive behavioral training on face-object associations on day 1 and then returned on day 2 for additional behavioral training, followed by an fMRI session, and finally a behavioral color memory test. A critical feature of our design is that we held color similarity between pairmates constant (24 degrees apart), but we included a &lt;strong&gt;competitive&lt;/strong&gt; and &lt;strong&gt;noncompetitive condition&lt;/strong&gt;. In the competitive condition, pairmate images corresponded to the same object category (e.g., two beanbags of slightly different colors). In the noncompetitive condition, pairmates corresponded to distinct object categories (e.g., a pillow and a ball of slightly different colors). Thus, in both conditions, the pairmates were 24 degrees apart in color space; but, for the competitive condition, color was the only feature dimension on which the pairmates differed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%201.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%201.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%202.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%202.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;h3 id=&#34;behavioral-performance-analysis&#34;&gt;Behavioral performance analysis&lt;/h3&gt;
&lt;p&gt;As for the result, first we found that participants exaggerated the color difference between the two similar objects for only for the competitive condition, not for the non-competitive condition. Moreover, the greater memory exaggeration was associated with lower memory interference (indicated by the better associative memory performance). See figures below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%203.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%203.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;neural-imaging-data-fmri-analysis&#34;&gt;Neural imaging data (fMRI) analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Image Data processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After pass the fMRI through the basic preprocessing pipeline, we smoothed the data with a 1.7 mm FWHM Gaussian kernel and high pass filtered at 0.01 Hz to increase signal-to-noise-ratio (&lt;strong&gt;SNR&lt;/strong&gt;). We modeled data with “least-squares separate” method. With this method, each item was estimated in a separate GLM as a separate regressor while all remaining items were modeled together with another regressor. The six movement parameters and framewise displacement were included in each GLM as confound regressors. This resulted in t maps that were used for the multivariate pattern analysis (&lt;strong&gt;MVPA&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%204.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%204.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neural representation of color information during recall&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As predicted, the greater relevance of color information in the competitive condition resulted in stronger representation of color information during recall, despite the fact that participants had not been explicitly oriented to color information in any way by this point of the experiment (the critical behavioral test of color memory occurred after fMRI scanning).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%205.png&#34; alt=&#34;Adaptive%20Memory%20Distortions%20Are%20Predicted%20by%20Featu%20ef556ea14bc84d6ab287f06f059abe81/Untitled%205.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neural measures of pairmate similarity predict color memory bias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moreover, we found only for the competitive condition, the more dissimilar vIPS activity patterns were when recalling pairmates, the greater the color memory repulsion effect for those pairmates. A mediation analysis performed at the level of individual pairmates also revealed that the relationship between vIPS dissimilarity and associative memory accuracy was significantly mediated by signed color memory distance, consistent with the interpretation that vIPS dissimilarity reflected the degree of color memory repulsion, which in turn was associated with better associative memory accuracy (lower interference).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Here, we show that competition between similar memories triggers biases in their neural representations and corresponding behavioral expressions. Specifically, we demonstrate that subtle, diagnostic differences between events were exaggerated in long-term memory and that this exaggeration reduced interference. Critically, these behavioral expressions of memory distortion were predicted by adaptive, feature-specific changes to memory representations in parietal cortex.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New &lt;a href=&#34;https://twitter.com/hashtag/JNeurosci?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#JNeurosci&lt;/a&gt; research from &lt;a href=&#34;https://twitter.com/_zhaoyufei?ref_src=twsrc%5Etfw&#34;&gt;@_zhaoyufei&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/AChanales?ref_src=twsrc%5Etfw&#34;&gt;@AChanales&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/KuhlLab?ref_src=twsrc%5Etfw&#34;&gt;@KuhlLab&lt;/a&gt; demonstrates that in order to remember similar events, the brain exaggerates the difference between them, resulting in divergent brain activity patterns but better memory performance.&lt;a href=&#34;https://t.co/JTFCrG4d5t&#34;&gt;https://t.co/JTFCrG4d5t&lt;/a&gt; &lt;a href=&#34;https://t.co/HkopLO1rkF&#34;&gt;pic.twitter.com/HkopLO1rkF&lt;/a&gt;&lt;/p&gt;&amp;mdash; SfN Journals (@SfNJournals) &lt;a href=&#34;https://twitter.com/SfNJournals/status/1363911724060000258?ref_src=twsrc%5Etfw&#34;&gt;February 22, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Visualize image set based on VGG16 Convolutional layer features</title>
      <link>/post/vgg-viusalization/</link>
      <pubDate>Mon, 16 Nov 2020 20:21:35 -0800</pubDate>
      <guid>/post/vgg-viusalization/</guid>
      <description>&lt;h2 id=&#34;in-this-notebook-i-am-going-to-show-you&#34;&gt;In this notebook, I am going to show you&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to extract the features for a set of images at a certain layer of VGG16 pretrained model with &lt;strong&gt;PyTorch&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to use &lt;strong&gt;PCA(Principle component analysis)&lt;/strong&gt; to reduce the feature dimension for better visualization.&lt;/li&gt;
&lt;li&gt;How to use &lt;strong&gt;t-SNE&lt;/strong&gt; to visualize the image set based on their similarity of layer features (visualize the n-dimension features as 2-d distance).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-set-for-example&#34;&gt;Image set for example&lt;/h2&gt;
&lt;p&gt;Here, I am using images from &lt;a href=&#34;http://naturalscenesdataset.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Scene Dataset&lt;/a&gt;. The Natural Scenes Dataset (NSD) is a large-scale fMRI dataset conducted at ultra-high-field (7T) strength at the Center of Magnetic Resonance Research (CMRR) at the University of Minnesota. The dataset consists of whole-brain, high-resolution (1.8-mm isotropic, 1.6-s sampling rate) fMRI measurements of 8 healthy adult subjects while they viewed thousands of color natural scenes over the course of 30–40 scan sessions. Images used in this dataset are originally from &lt;a href=&#34;https://cocodataset.org/#home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft COCO&lt;/a&gt;. I am working on semantic and memory-related exploratory analysis about this dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pathlib import Path
import h5py
from torchvision import models
from PIL import Image
from torchvision import transforms
import numpy as np
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;read-in-the-image&#34;&gt;Read in the image&lt;/h3&gt;
&lt;p&gt;NSD images are square-cropped images from Microsoft Coco dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import stimuli
sti_dir = Path(&#39;/projects/hulacon/shared/nsd/nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5&#39;).as_posix()
sti = h5py.File(sti_dir,&#39;r&#39;)
sti_array = sti[&#39;imgBrick&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;read-in-the-pretrianed-vgg16-model&#34;&gt;Read in the pretrianed VGG16 model&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import model
pretrained_model = models.vgg16(pretrained=True).features
pretrained_model.eval()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace=True)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace=True)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace=True)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace=True)
  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (18): ReLU(inplace=True)
  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace=True)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace=True)
  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (25): ReLU(inplace=True)
  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (27): ReLU(inplace=True)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace=True)
  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Above we can see the convolutional layer structure of VGG. The index of each layer is labeled. I will use the pooling layer 1 as the example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conv_layer = {
    &#39;conv1&#39;: 0,
    &#39;conv2&#39;: 2,
    &#39;conv3&#39;: 5,
    &#39;conv4&#39;: 7,
    &#39;conv5&#39;: 10,
    &#39;conv6&#39;: 12,
    &#39;conv7&#39;: 14,
    &#39;conv8&#39;: 17,
    &#39;conv9&#39;: 19,
    &#39;conv10&#39;: 21,
    &#39;conv11&#39;: 24,
    &#39;conv12&#39;: 26,
    &#39;conv13&#39;: 28}

pooling_layer = {
    &#39;pool1&#39;: 4,
    &#39;pool2&#39;: 9,
    &#39;pool3&#39;: 16,
    &#39;pool4&#39;: 23,
    &#39;pool5&#39;: 30}

selected_layer = pooling_layer[&#39;pool1&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;selected_layer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;image-preprocessing&#34;&gt;Image preprocessing&lt;/h3&gt;
&lt;p&gt;Here, we define the function that can normailize the input images.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# define preprocess parameters
# mini-batches of 3-channel RGB images of shape (3 x H x W)
preprocess = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def image_preprocess(img):
    &amp;quot;&amp;quot;&amp;quot;
    Preprocess the image and create the tensor
    &amp;quot;&amp;quot;&amp;quot;
    im = Image.fromarray(img)
    input_tensor = preprocess(im) # create tensor
    input_batch = input_tensor.unsqueeze(0)# create a mini-batch as expected by the model
    return input_batch
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;Feature extraction&lt;/h3&gt;
&lt;p&gt;Here, we define the function that can extract features at a centain layer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def feature_extract(tensor, selected_layer):
    for index,layer in enumerate(pretrained_model):
#         print(index, layer)
        tensor = layer(tensor)
        if (index == selected_layer):
            return tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;read-in-and-preprocess-pictures&#34;&gt;Read in and preprocess pictures&lt;/h3&gt;
&lt;p&gt;We are going to use 200 pictures from the stimuli set as an example. We flatten all the feature for each image in order to calculate distance between image features/ use t-SNE&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;features = []
pic_number = 200
for iImage in range(pic_number):
    img = sti_array[iImage,:,:,:]
    input_batch = image_preprocess(img)
    current_features = np.squeeze(feature_extract(input_batch, selected_layer).data.numpy())
#     print(current_features.shape)
    features.append(np.concatenate(current_features, axis=None))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;features = np.array(features)
features.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(200, 802816)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are using the pooling layer, the feature numbers aren&amp;rsquo;t crazily large. We can skip the following steps for PCA.
If you are extracting features from convolutional layers, PCA would be very helpful.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# from sklearn.decomposition import PCA

# 
# pca = PCA(n_components=1000)
# pca.fit(features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pca_features = pca.transform(features)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;t-sne-visualization&#34;&gt;t-SNE visualization&lt;/h3&gt;
&lt;p&gt;Here, we are just using some default hyperparameters for t-SNE for a simple visualization. You can try to tune the hyperparameters if you like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.manifold import TSNE
images = sti_array[0:pic_number,:,:,:]
features = np.array(features)
tsne = TSNE(n_components=2, learning_rate=150, perplexity=30, angle=0.2, verbose=2).fit_transform(features)
tx, ty = tsne[:,0], tsne[:,1]
tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))
ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[t-SNE] Computing 91 nearest neighbors...
[t-SNE] Indexed 200 samples in 4.441s...
[t-SNE] Computed neighbors for 200 samples in 45.717s...
[t-SNE] Computed conditional probabilities for sample 200 / 200
[t-SNE] Mean sigma: 321.907110
[t-SNE] Computed conditional probabilities in 0.021s
[t-SNE] Iteration 50: error = 110.1199646, gradient norm = 0.3914785 (50 iterations in 0.065s)
[t-SNE] Iteration 100: error = 119.6677475, gradient norm = 0.2513522 (50 iterations in 0.065s)
[t-SNE] Iteration 150: error = 120.5060120, gradient norm = 0.2420261 (50 iterations in 0.063s)
[t-SNE] Iteration 200: error = 117.5590820, gradient norm = 0.4199304 (50 iterations in 0.063s)
[t-SNE] Iteration 250: error = 122.9965286, gradient norm = 0.2279717 (50 iterations in 0.060s)
[t-SNE] KL divergence after 250 iterations with early exaggeration: 122.996529
[t-SNE] Iteration 300: error = 2.4189911, gradient norm = 0.0285375 (50 iterations in 0.060s)
[t-SNE] Iteration 350: error = 1.8055801, gradient norm = 0.0019180 (50 iterations in 0.062s)
[t-SNE] Iteration 400: error = 1.6958097, gradient norm = 0.0008569 (50 iterations in 0.062s)
[t-SNE] Iteration 450: error = 1.6566454, gradient norm = 0.0006347 (50 iterations in 0.061s)
[t-SNE] Iteration 500: error = 1.6307240, gradient norm = 0.0004043 (50 iterations in 0.061s)
[t-SNE] Iteration 550: error = 1.6122439, gradient norm = 0.0003583 (50 iterations in 0.060s)
[t-SNE] Iteration 600: error = 1.5946932, gradient norm = 0.0002276 (50 iterations in 0.059s)
[t-SNE] Iteration 650: error = 1.5851456, gradient norm = 0.0002688 (50 iterations in 0.057s)
[t-SNE] Iteration 700: error = 1.5735952, gradient norm = 0.0011064 (50 iterations in 0.057s)
[t-SNE] Iteration 750: error = 1.5616177, gradient norm = 0.0003782 (50 iterations in 0.057s)
[t-SNE] Iteration 800: error = 1.5549071, gradient norm = 0.0001757 (50 iterations in 0.058s)
[t-SNE] Iteration 850: error = 1.5460689, gradient norm = 0.0002423 (50 iterations in 0.057s)
[t-SNE] Iteration 900: error = 1.5415170, gradient norm = 0.0001737 (50 iterations in 0.057s)
[t-SNE] Iteration 950: error = 1.5383173, gradient norm = 0.0000928 (50 iterations in 0.057s)
[t-SNE] Iteration 1000: error = 1.5367311, gradient norm = 0.0000665 (50 iterations in 0.059s)
[t-SNE] KL divergence after 1000 iterations: 1.536731
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib.pyplot import imshow
import matplotlib
width = 4000
height = 3000
max_dim = 100

full_image = Image.new(&#39;RGBA&#39;, (width, height))
for img, x, y in zip(images, tx, ty):
    tile = Image.fromarray(img)
    rs = max(1, tile.width/max_dim, tile.height/max_dim)
    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)
    full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert(&#39;RGBA&#39;))

matplotlib.pyplot.figure(figsize = (16,12))
imshow(full_image)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x2aab23e3dcf8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./Conv_extract_22_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can see clear color and shape clusters in the visualization. This makes sense since the pooling layer 1 is at a very early stage, which caputres relatively low level features of the images.&lt;/p&gt;
&lt;p&gt;In below, the visulation based on pooling layer 5 is presented.
You can clearly see some semantic category clusters, like bears and bananas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(filename=&#39;samples/pool5_600pics_try2.png&#39;) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./Conv_extract_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/utkuozbulak/pytorch-cnn-visualizations&#34;&gt;https://github.com/utkuozbulak/pytorch-cnn-visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nextjournal.com/ml4a/image-t-sne&#34;&gt;https://nextjournal.com/ml4a/image-t-sne&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Books/video courses recommendation for data science related coding/machine learning/stats</title>
      <link>/post/ref-data-science/</link>
      <pubDate>Fri, 02 Oct 2020 21:30:53 -0800</pubDate>
      <guid>/post/ref-data-science/</guid>
      <description>&lt;p&gt;Although the theory basis of my major, cognitive neuroscience, is based on psychology and biology, the skills I need to tackle with the research work of this field are most data science related. Since programming languages and data related techniques are  developing with dramatically speed, staying tuned and keeping studying is a must.&lt;/p&gt;
&lt;p&gt;Here I want to list some books/course videos that I found super helpful along my learning path.&lt;/p&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greenteapress.com/wp/think-python-2e/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Think Python&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;I learned Python from beginning with this book.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/fluent-python/9781491946237/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fluent Python&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;I learned many advanced skills in python from this book. Python has so many fancy functions/tricks that can be easily missed. This book help improving your codes in a more elegant and efficient way.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/python-cookbook-3rd/9781449357337/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python Cookbook&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;A book with many random skills and tricks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python Data Science Handbook&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Learned Numpy, Pandas, Scikit-learn with this book.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r4ds.had.co.nz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R for Data Science&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Everything you need to know about tidyverse is here.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://adv-r.hadley.nz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced R&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Programming skills with R.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r-graphics.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Graphics Cookbook&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Fantastic book for ggplot2 lovers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r-pkgs.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Packages&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Built my first R package with this awesome book.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blogdown: Creating Websites with R Markdown&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Interested in building a website like this one you are looking at? Check out this book.&lt;/li&gt;
&lt;li&gt;I also found &lt;a href=&#34;https://bookdown.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bookdown&lt;/a&gt; and &lt;a href=&#34;https://pagedown.rbind.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pagedown&lt;/a&gt; are so helpful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs229.stanford.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS229: Machine Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This machine learning is provided by Andrew Ng from Stanford. Course videos can be found via Youtube. Very comprehensive introduction to the basic knowledge of machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.stanford.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;As the course title implicates, this course is focused on how to apply CNN. In general it is quite straight forward if you have a solid knowledge base from CS229. Course videos can also be found via Youtube.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deeplearning.ai&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Andrew Ng also provides a series of advance deep learning course via Coursera.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statlearning.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;I read this book while taking the machine learning course for my &lt;a href=&#34;https://github.com/uo-datasci-specialization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Specialization&lt;/a&gt; at the University of Oregon. Very detailed and clear explanation for both the stats and coding of statistical learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Elements of Statistical Learning: Data Mining, Inference, and Prediction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;More advanced comparing with the previous one.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Intro to &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/CSS_basics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSS&lt;/a&gt; and &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A practical video tutorial for Illustrator &lt;a href=&#34;https://www.youtube.com/playlist?list=PLnLzAhQDUqEC_yMyn_QvMzUgGykJkvI0F&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adobe Illustrator Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pro Git&lt;/a&gt; something about version control&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/unix-and-linux/9780133793871/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unix and Linux Visual QuickStart Guide&lt;/a&gt; learning a bit unix will even change your experience with your pc.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/mumfordbrainstats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mumford Brain Stats&lt;/a&gt; Stats knowledge in neuroscience area.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/project_landing/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project_landing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/example/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gallery</title>
      <link>/gallery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/gallery/</guid>
      <description>&lt;style&gt;
.article-container {
  max-width: 80%;
  padding: 0 20px 0 20px;
  margin: 0 auto 0 auto;
}
&lt;/style&gt;
&lt;p&gt;As an old Chinese saying goes, &amp;ldquo;Life is about reading ten thousands of books and traveling ten thousands of miles.&amp;rdquo; (读万卷书 行万里路). Besides reading, in my spare time I love traveling around the world. So far I have been to Japan, Thailand, Taiwan, France, Portugal, Spain, and of course, many cities in main-land China and the U.S., the two countries I have been spending most of my time with my beloved family and friends.








  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC00669.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC00669_hu762d776ab8211792b51ca28299762ea6_9863168_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC00669.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC00709.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC00709_hu446cb6985aee05e6bec9acc20d90f90a_7897088_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC00709.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC01010.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC01010_hu69a9e6d36a12fc6079cc41ab999900e7_11206656_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC01010.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC01183.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC01183_huc954640b48d03c12a1cef991b43c9ca2_5013504_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC01183.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC01225.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC01225_hu036788ac257cb52476c6e38121cbc0db_10354688_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC01225.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC01633.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC01633_huc3bd216d39d53509f3d71e9787f9e8ae_8126464_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC01633.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC01693.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC01693_hu7a89739c3b543ac9fc252e87ea71b766_8323072_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC01693.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/DSC02145.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/DSC02145_hucaa6b6794e008df7939a25b72c471f81_8224768_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;DSC02145.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28142%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28142%29_huc692529ee93bcb44e3725811cf072e67_3156553_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(142).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28168%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28168%29_hu229b438d6a33e23c635c58ff9cb8b551_2808622_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(168).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28237%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28237%29_hufd1dc9a049cc7464e88907c789ac85b1_1432363_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(237).jpg&#34; width=&#34;190&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%283%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%283%29_hu5b29fc15175103206230588428f4ee7b_2859674_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(3).jpg&#34; width=&#34;190&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28305%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28305%29_hu07dcd07bf4b8ad743c77f2a3794b53c8_2648044_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(305).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28419%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28419%29_hub3db65a51d05236d46cea0fa3e078414_2625712_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(419).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%28422%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%28422%29_hu8ba491fddda8d8184954bab5e37492c8_2190970_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(422).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/FullSizeRender%2883%29.jpg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/FullSizeRender%2883%29_hu2a4db6310c7305f7ce6523d54bc620c9_1466146_0x190_resize_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;FullSizeRender(83).jpg&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/IMG_4313.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/IMG_4313_hucd0b4cf59442629ab80c3281a0dfc818_2904040_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;IMG_4313.JPG&#34; width=&#34;143&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/IMG_4351.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/IMG_4351_hubf947dce646d65ce89dd486f7c7ab64d_2511748_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;IMG_4351.JPG&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/ORG_DSC01610.JPG&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/ORG_DSC01610_hu18909217972403f64da680a3a4e55ef8_4063232_0x190_resize_q75_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;ORG_DSC01610.JPG&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/WechatIMG1030.jpeg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/WechatIMG1030_hu638fba828529f41b83498f164edcadcb_11567104_0x190_resize_q75_lanczos.jpeg&#34; class=&#34;lazyload&#34; alt=&#34;WechatIMG1030.jpeg&#34; width=&#34;285&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
    &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;/gallery/gallery/WechatIMG234.jpeg&#34; &gt;
      &lt;img data-src=&#34;/gallery/gallery/WechatIMG234_huf667685d46a901d4088b2ea4a80fcac4_6317143_0x190_resize_q75_lanczos.jpeg&#34; class=&#34;lazyload&#34; alt=&#34;WechatIMG234.jpeg&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

  
&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
